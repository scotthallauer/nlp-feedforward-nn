NGRAM_SIZE = 6
EMBEDDING_DIM = 10
BATCH_SIZE = 128
INITIAL_LEARNING_RATE = 0.1

Start Training
| epoch 1 | 200/ 5122 batches | lr 0.1000 | ms/batch 2.90 | loss 10.67 | ppl 44334.65
| epoch 1 | 400/ 5122 batches | lr 0.1000 | ms/batch 2.80 | loss 10.30 | ppl 30374.07
| epoch 1 | 600/ 5122 batches | lr 0.1000 | ms/batch 2.77 | loss 10.16 | ppl 26529.80
| epoch 1 | 800/ 5122 batches | lr 0.1000 | ms/batch 2.78 | loss 10.05 | ppl 23655.25
| epoch 1 | 1000/ 5122 batches | lr 0.1000 | ms/batch 2.78 | loss 9.88 | ppl 19981.79
| epoch 1 | 1200/ 5122 batches | lr 0.1000 | ms/batch 2.79 | loss 9.79 | ppl 18361.36
| epoch 1 | 1400/ 5122 batches | lr 0.1000 | ms/batch 2.79 | loss 9.66 | ppl 16183.53
| epoch 1 | 1600/ 5122 batches | lr 0.1000 | ms/batch 2.79 | loss 9.59 | ppl 15105.13
| epoch 1 | 1800/ 5122 batches | lr 0.1000 | ms/batch 2.80 | loss 9.50 | ppl 13684.33
| epoch 1 | 2000/ 5122 batches | lr 0.1000 | ms/batch 2.78 | loss 9.45 | ppl 13023.97
| epoch 1 | 2200/ 5122 batches | lr 0.1000 | ms/batch 2.76 | loss 9.41 | ppl 12665.54
| epoch 1 | 2400/ 5122 batches | lr 0.1000 | ms/batch 2.79 | loss 9.39 | ppl 12276.90
| epoch 1 | 2600/ 5122 batches | lr 0.1000 | ms/batch 2.86 | loss 9.31 | ppl 11467.41
| epoch 1 | 2800/ 5122 batches | lr 0.1000 | ms/batch 2.77 | loss 9.27 | ppl 10892.26
| epoch 1 | 3000/ 5122 batches | lr 0.1000 | ms/batch 2.78 | loss 9.27 | ppl 10910.71
| epoch 1 | 3200/ 5122 batches | lr 0.1000 | ms/batch 2.79 | loss 9.22 | ppl 10472.32
| epoch 1 | 3400/ 5122 batches | lr 0.1000 | ms/batch 2.78 | loss 9.15 | ppl 9767.41
| epoch 1 | 3600/ 5122 batches | lr 0.1000 | ms/batch 2.77 | loss 9.14 | ppl 9629.13
| epoch 1 | 3800/ 5122 batches | lr 0.1000 | ms/batch 2.78 | loss 9.12 | ppl 9400.65
| epoch 1 | 4000/ 5122 batches | lr 0.1000 | ms/batch 2.80 | loss 9.11 | ppl 9329.51
| epoch 1 | 4200/ 5122 batches | lr 0.1000 | ms/batch 2.80 | loss 9.09 | ppl 9113.96
| epoch 1 | 4400/ 5122 batches | lr 0.1000 | ms/batch 2.77 | loss 9.04 | ppl 8685.50
| epoch 1 | 4600/ 5122 batches | lr 0.1000 | ms/batch 2.77 | loss 9.08 | ppl 9111.62
| epoch 1 | 4800/ 5122 batches | lr 0.1000 | ms/batch 2.81 | loss 9.03 | ppl 8667.40
| epoch 1 | 5000/ 5122 batches | lr 0.1000 | ms/batch 2.78 | loss 9.05 | ppl 8776.96
-------------------------------------------------------------------
| end of epoch 1 | time: 56.68s | valid loss 8.21 | valid ppl 3849.94
-------------------------------------------------------------------
Start Training
| epoch 2 | 200/ 5122 batches | lr 0.0900 | ms/batch 2.80 | loss 9.01 | ppl 8379.34
| epoch 2 | 400/ 5122 batches | lr 0.0900 | ms/batch 2.78 | loss 8.99 | ppl 8289.70
| epoch 2 | 600/ 5122 batches | lr 0.0900 | ms/batch 2.79 | loss 8.97 | ppl 8160.13
| epoch 2 | 800/ 5122 batches | lr 0.0900 | ms/batch 2.87 | loss 8.98 | ppl 8145.94
| epoch 2 | 1000/ 5122 batches | lr 0.0900 | ms/batch 2.77 | loss 8.96 | ppl 8035.66
| epoch 2 | 1200/ 5122 batches | lr 0.0900 | ms/batch 2.77 | loss 8.96 | ppl 8014.29
| epoch 2 | 1400/ 5122 batches | lr 0.0900 | ms/batch 2.81 | loss 8.95 | ppl 7984.24
| epoch 2 | 1600/ 5122 batches | lr 0.0900 | ms/batch 2.82 | loss 8.96 | ppl 8031.26
| epoch 2 | 1800/ 5122 batches | lr 0.0900 | ms/batch 2.81 | loss 8.96 | ppl 8067.16
| epoch 2 | 2000/ 5122 batches | lr 0.0900 | ms/batch 2.76 | loss 8.96 | ppl 8019.29
| epoch 2 | 2200/ 5122 batches | lr 0.0900 | ms/batch 2.81 | loss 8.94 | ppl 7902.47
| epoch 2 | 2400/ 5122 batches | lr 0.0900 | ms/batch 2.78 | loss 8.94 | ppl 7890.72
| epoch 2 | 2600/ 5122 batches | lr 0.0900 | ms/batch 2.76 | loss 8.95 | ppl 7981.18
| epoch 2 | 2800/ 5122 batches | lr 0.0900 | ms/batch 2.76 | loss 8.93 | ppl 7802.56
| epoch 2 | 3000/ 5122 batches | lr 0.0900 | ms/batch 2.79 | loss 8.92 | ppl 7731.04
| epoch 2 | 3200/ 5122 batches | lr 0.0900 | ms/batch 2.78 | loss 8.97 | ppl 8079.55
| epoch 2 | 3400/ 5122 batches | lr 0.0900 | ms/batch 2.79 | loss 8.92 | ppl 7680.79
| epoch 2 | 3600/ 5122 batches | lr 0.0900 | ms/batch 2.80 | loss 8.92 | ppl 7728.70
| epoch 2 | 3800/ 5122 batches | lr 0.0900 | ms/batch 2.78 | loss 8.91 | ppl 7592.72
| epoch 2 | 4000/ 5122 batches | lr 0.0900 | ms/batch 2.80 | loss 8.92 | ppl 7689.81
| epoch 2 | 4200/ 5122 batches | lr 0.0900 | ms/batch 2.77 | loss 8.93 | ppl 7756.95
| epoch 2 | 4400/ 5122 batches | lr 0.0900 | ms/batch 2.77 | loss 8.91 | ppl 7641.46
| epoch 2 | 4600/ 5122 batches | lr 0.0900 | ms/batch 2.79 | loss 8.89 | ppl 7488.28
| epoch 2 | 4800/ 5122 batches | lr 0.0900 | ms/batch 2.76 | loss 8.89 | ppl 7505.99
| epoch 2 | 5000/ 5122 batches | lr 0.0900 | ms/batch 2.78 | loss 8.93 | ppl 7830.70
-------------------------------------------------------------------
| end of epoch 2 | time: 56.77s | valid loss 8.12 | valid ppl 3511.04
-------------------------------------------------------------------
Start Training
| epoch 3 | 200/ 5122 batches | lr 0.0810 | ms/batch 2.78 | loss 8.87 | ppl 7337.18
| epoch 3 | 400/ 5122 batches | lr 0.0810 | ms/batch 2.81 | loss 8.87 | ppl 7247.52
| epoch 3 | 600/ 5122 batches | lr 0.0810 | ms/batch 2.79 | loss 8.85 | ppl 7167.79
| epoch 3 | 800/ 5122 batches | lr 0.0810 | ms/batch 2.77 | loss 8.86 | ppl 7250.34
| epoch 3 | 1000/ 5122 batches | lr 0.0810 | ms/batch 2.79 | loss 8.85 | ppl 7177.52
| epoch 3 | 1200/ 5122 batches | lr 0.0810 | ms/batch 2.79 | loss 8.86 | ppl 7188.03
| epoch 3 | 1400/ 5122 batches | lr 0.0810 | ms/batch 2.79 | loss 8.84 | ppl 7156.43
| epoch 3 | 1600/ 5122 batches | lr 0.0810 | ms/batch 2.81 | loss 8.85 | ppl 7126.78
| epoch 3 | 1800/ 5122 batches | lr 0.0810 | ms/batch 2.79 | loss 8.88 | ppl 7371.23
| epoch 3 | 2000/ 5122 batches | lr 0.0810 | ms/batch 2.79 | loss 8.85 | ppl 7235.06
| epoch 3 | 2200/ 5122 batches | lr 0.0810 | ms/batch 2.78 | loss 8.84 | ppl 7155.87
| epoch 3 | 2400/ 5122 batches | lr 0.0810 | ms/batch 2.80 | loss 8.84 | ppl 7105.13
| epoch 3 | 2600/ 5122 batches | lr 0.0810 | ms/batch 2.78 | loss 8.85 | ppl 7179.03
| epoch 3 | 2800/ 5122 batches | lr 0.0810 | ms/batch 2.82 | loss 8.87 | ppl 7356.81
| epoch 3 | 3000/ 5122 batches | lr 0.0810 | ms/batch 2.80 | loss 8.85 | ppl 7184.91
| epoch 3 | 3200/ 5122 batches | lr 0.0810 | ms/batch 2.82 | loss 8.83 | ppl 7093.13
| epoch 3 | 3400/ 5122 batches | lr 0.0810 | ms/batch 2.80 | loss 8.81 | ppl 6960.48
| epoch 3 | 3600/ 5122 batches | lr 0.0810 | ms/batch 2.78 | loss 8.83 | ppl 7068.25
| epoch 3 | 3800/ 5122 batches | lr 0.0810 | ms/batch 2.79 | loss 8.83 | ppl 7065.18
| epoch 3 | 4000/ 5122 batches | lr 0.0810 | ms/batch 2.80 | loss 8.85 | ppl 7212.90
| epoch 3 | 4200/ 5122 batches | lr 0.0810 | ms/batch 2.81 | loss 8.83 | ppl 7009.90
| epoch 3 | 4400/ 5122 batches | lr 0.0810 | ms/batch 2.78 | loss 8.86 | ppl 7256.68
| epoch 3 | 4600/ 5122 batches | lr 0.0810 | ms/batch 2.80 | loss 8.85 | ppl 7183.33
| epoch 3 | 4800/ 5122 batches | lr 0.0810 | ms/batch 2.82 | loss 8.84 | ppl 7107.29
| epoch 3 | 5000/ 5122 batches | lr 0.0810 | ms/batch 2.78 | loss 8.79 | ppl 6796.58
-------------------------------------------------------------------
| end of epoch 3 | time: 56.96s | valid loss 8.10 | valid ppl 3432.59
-------------------------------------------------------------------
Start Training
| epoch 4 | 200/ 5122 batches | lr 0.0729 | ms/batch 2.76 | loss 8.81 | ppl 6902.62
| epoch 4 | 400/ 5122 batches | lr 0.0729 | ms/batch 2.80 | loss 8.81 | ppl 6916.17
| epoch 4 | 600/ 5122 batches | lr 0.0729 | ms/batch 2.78 | loss 8.78 | ppl 6703.40
| epoch 4 | 800/ 5122 batches | lr 0.0729 | ms/batch 2.80 | loss 8.79 | ppl 6738.63
| epoch 4 | 1000/ 5122 batches | lr 0.0729 | ms/batch 2.79 | loss 8.77 | ppl 6628.09
| epoch 4 | 1200/ 5122 batches | lr 0.0729 | ms/batch 2.79 | loss 8.80 | ppl 6836.51
| epoch 4 | 1400/ 5122 batches | lr 0.0729 | ms/batch 2.77 | loss 8.78 | ppl 6713.24
| epoch 4 | 1600/ 5122 batches | lr 0.0729 | ms/batch 2.79 | loss 8.81 | ppl 6942.05
| epoch 4 | 1800/ 5122 batches | lr 0.0729 | ms/batch 2.79 | loss 8.79 | ppl 6798.93
| epoch 4 | 2000/ 5122 batches | lr 0.0729 | ms/batch 2.78 | loss 8.78 | ppl 6751.13
| epoch 4 | 2200/ 5122 batches | lr 0.0729 | ms/batch 2.78 | loss 8.80 | ppl 6854.85
| epoch 4 | 2400/ 5122 batches | lr 0.0729 | ms/batch 2.86 | loss 8.80 | ppl 6829.67
| epoch 4 | 2600/ 5122 batches | lr 0.0729 | ms/batch 2.81 | loss 8.79 | ppl 6749.25
| epoch 4 | 2800/ 5122 batches | lr 0.0729 | ms/batch 2.77 | loss 8.76 | ppl 6572.97
| epoch 4 | 3000/ 5122 batches | lr 0.0729 | ms/batch 2.77 | loss 8.78 | ppl 6747.25
| epoch 4 | 3200/ 5122 batches | lr 0.0729 | ms/batch 2.82 | loss 8.77 | ppl 6679.93
| epoch 4 | 3400/ 5122 batches | lr 0.0729 | ms/batch 2.78 | loss 8.75 | ppl 6549.27
| epoch 4 | 3600/ 5122 batches | lr 0.0729 | ms/batch 2.84 | loss 8.78 | ppl 6697.64
| epoch 4 | 3800/ 5122 batches | lr 0.0729 | ms/batch 2.81 | loss 8.77 | ppl 6662.31
| epoch 4 | 4000/ 5122 batches | lr 0.0729 | ms/batch 2.82 | loss 8.76 | ppl 6609.32
| epoch 4 | 4200/ 5122 batches | lr 0.0729 | ms/batch 2.81 | loss 8.78 | ppl 6717.13
| epoch 4 | 4400/ 5122 batches | lr 0.0729 | ms/batch 2.80 | loss 8.76 | ppl 6587.26
| epoch 4 | 4600/ 5122 batches | lr 0.0729 | ms/batch 2.80 | loss 8.78 | ppl 6689.51
| epoch 4 | 4800/ 5122 batches | lr 0.0729 | ms/batch 2.80 | loss 8.80 | ppl 6863.79
| epoch 4 | 5000/ 5122 batches | lr 0.0729 | ms/batch 2.79 | loss 8.74 | ppl 6449.54
-------------------------------------------------------------------
| end of epoch 4 | time: 57.01s | valid loss 8.05 | valid ppl 3272.90
-------------------------------------------------------------------
Start Training
| epoch 5 | 200/ 5122 batches | lr 0.0656 | ms/batch 2.79 | loss 8.75 | ppl 6533.97
| epoch 5 | 400/ 5122 batches | lr 0.0656 | ms/batch 2.76 | loss 8.73 | ppl 6401.47
| epoch 5 | 600/ 5122 batches | lr 0.0656 | ms/batch 2.76 | loss 8.71 | ppl 6244.47
| epoch 5 | 800/ 5122 batches | lr 0.0656 | ms/batch 2.78 | loss 8.76 | ppl 6563.71
| epoch 5 | 1000/ 5122 batches | lr 0.0656 | ms/batch 2.85 | loss 8.71 | ppl 6303.75
| epoch 5 | 1200/ 5122 batches | lr 0.0656 | ms/batch 2.76 | loss 8.75 | ppl 6475.97
| epoch 5 | 1400/ 5122 batches | lr 0.0656 | ms/batch 2.76 | loss 8.75 | ppl 6480.74
| epoch 5 | 1600/ 5122 batches | lr 0.0656 | ms/batch 2.78 | loss 8.75 | ppl 6525.19
| epoch 5 | 1800/ 5122 batches | lr 0.0656 | ms/batch 2.80 | loss 8.72 | ppl 6279.48
| epoch 5 | 2000/ 5122 batches | lr 0.0656 | ms/batch 2.79 | loss 8.74 | ppl 6451.02
| epoch 5 | 2200/ 5122 batches | lr 0.0656 | ms/batch 2.78 | loss 8.74 | ppl 6462.32
| epoch 5 | 2400/ 5122 batches | lr 0.0656 | ms/batch 2.79 | loss 8.75 | ppl 6494.08
| epoch 5 | 2600/ 5122 batches | lr 0.0656 | ms/batch 2.81 | loss 8.73 | ppl 6372.97
| epoch 5 | 2800/ 5122 batches | lr 0.0656 | ms/batch 2.82 | loss 8.73 | ppl 6385.25
| epoch 5 | 3000/ 5122 batches | lr 0.0656 | ms/batch 2.77 | loss 8.71 | ppl 6253.06
| epoch 5 | 3200/ 5122 batches | lr 0.0656 | ms/batch 2.78 | loss 8.73 | ppl 6381.61
| epoch 5 | 3400/ 5122 batches | lr 0.0656 | ms/batch 2.82 | loss 8.72 | ppl 6318.15
| epoch 5 | 3600/ 5122 batches | lr 0.0656 | ms/batch 2.80 | loss 8.74 | ppl 6406.93
| epoch 5 | 3800/ 5122 batches | lr 0.0656 | ms/batch 2.81 | loss 8.72 | ppl 6302.44
| epoch 5 | 4000/ 5122 batches | lr 0.0656 | ms/batch 2.79 | loss 8.71 | ppl 6279.18
| epoch 5 | 4200/ 5122 batches | lr 0.0656 | ms/batch 2.80 | loss 8.72 | ppl 6286.79
| epoch 5 | 4400/ 5122 batches | lr 0.0656 | ms/batch 2.79 | loss 8.71 | ppl 6236.56
| epoch 5 | 4600/ 5122 batches | lr 0.0656 | ms/batch 2.80 | loss 8.70 | ppl 6237.76
| epoch 5 | 4800/ 5122 batches | lr 0.0656 | ms/batch 2.78 | loss 8.69 | ppl 6142.42
| epoch 5 | 5000/ 5122 batches | lr 0.0656 | ms/batch 2.79 | loss 8.73 | ppl 6372.92
-------------------------------------------------------------------
| end of epoch 5 | time: 56.81s | valid loss 8.06 | valid ppl 3311.18
-------------------------------------------------------------------
=========================================================================================
| End of training | test loss 8.06 | test ppl 3309.16
=========================================================================================
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp-feedforward-nn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQJwOwG5ApUg"
      },
      "source": [
        "# NLP Assignment 2\n",
        "\n",
        "**Authors:** Scott Hallauer (HLLSCO001) and Steve Wang (WNGSHU003)\n",
        "\n",
        "**Date:** 21 June 2021\n",
        "\n",
        "\n",
        "Please see the associated README.md file for instructions on how to run this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TZir7sSW_7z"
      },
      "source": [
        "## Set-Up Environment\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YACofn5oRF8W"
      },
      "source": [
        "Import all required libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gr-77pCjAnKz"
      },
      "source": [
        "import urllib\n",
        "import re\n",
        "import datetime\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.optim.lr_scheduler import ExponentialLR"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zp-QRNj2RJaH"
      },
      "source": [
        "Set the model's training hyperparameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXppOZ-zRL1Z"
      },
      "source": [
        "# Manual parameters\n",
        "NGRAM_SIZE = 6\n",
        "EMBEDDING_DIM = 10\n",
        "BATCH_SIZE = 64\n",
        "INITIAL_LEARNING_RATE = 0.1\n",
        "\n",
        "# Calculated parameters\n",
        "CONTEXT_SIZE = NGRAM_SIZE-1"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0hRbi_SS_lW"
      },
      "source": [
        "Set the URLs for where the isiXhosa datasets are stored."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiPZG9y6g_sW"
      },
      "source": [
        "train_url = \"https://raw.githubusercontent.com/scotthallauer/nlp-feedforward-nn/main/xh-data/nchlt_text.xh.train\"\n",
        "valid_url = \"https://raw.githubusercontent.com/scotthallauer/nlp-feedforward-nn/main/xh-data/nchlt_text.xh.valid\"\n",
        "test_url = \"https://raw.githubusercontent.com/scotthallauer/nlp-feedforward-nn/main/xh-data/nchlt_text.xh.test\""
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "priIoyy_EKrO"
      },
      "source": [
        "## Prepare Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lDYnEzdTSsV"
      },
      "source": [
        "Define helper functions for processing the raw datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K63_zdwFEOko"
      },
      "source": [
        "def load_dataset(url):\n",
        "  dataset = []\n",
        "  with urllib.request.urlopen(url) as data:\n",
        "    for line in data:\n",
        "      input_sentence = line.decode().split()\n",
        "      output_sentence = []\n",
        "      for input_word in input_sentence:\n",
        "        output_word = input_word.lower()\n",
        "        output_word = re.sub(r\"(\\.|\\,|\\:|\\?|\\)|\\\"|\\-|\\!|\\/|\\')+$\", \"\", output_word)\n",
        "        output_word = re.sub(r\"^(\\(|\\\"|\\-|\\/|\\')+\", \"\", output_word)\n",
        "        if len(output_word) > 0:\n",
        "          output_sentence.append(output_word)\n",
        "      dataset.append(output_sentence)\n",
        "  return dataset\n",
        "\n",
        "def prepare_train_dataset(dataset):\n",
        "  word_counts = {}\n",
        "  for sentence in dataset:\n",
        "    for word in sentence:\n",
        "      if word not in word_counts:\n",
        "        word_counts[word] = 1\n",
        "      else:\n",
        "        word_counts[word] += 1\n",
        "  for i in range(len(dataset)):\n",
        "    for j in range(len(dataset[i])):\n",
        "      if word_counts[dataset[i][j]] == 1:\n",
        "        dataset[i][j] = \"<unk>\"\n",
        "  return dataset\n",
        "\n",
        "def prepare_eval_dataset(dataset, train_word_index):\n",
        "  for i in range(len(dataset)):\n",
        "    for j in range(len(dataset[i])):\n",
        "      if dataset[i][j] not in train_word_index:\n",
        "        dataset[i][j] = \"<unk>\"\n",
        "  return dataset\n",
        "\n",
        "def get_word_index(sentences):\n",
        "  word_index = {}\n",
        "  index = 0\n",
        "  for sentence in sentences:\n",
        "    for word in sentence:\n",
        "      if word not in word_index:\n",
        "        word_index[word] = index\n",
        "        index += 1\n",
        "  return word_index\n",
        "\n",
        "def get_vocab(word_index):\n",
        "  vocab = []\n",
        "  for i in range(len(word_index)):\n",
        "    vocab.append(\"\");\n",
        "  for word in word_index:\n",
        "    vocab[word_index[word]] = word\n",
        "  return vocab\n",
        "\n",
        "def get_ngrams(sentences, n):\n",
        "  ngrams = []\n",
        "  for sentence in sentences:\n",
        "    if len(sentence) >= n:\n",
        "      for i in range(len(sentence)-(n-1)):\n",
        "        ngrams.append((sentence[i:i+(n-1)], sentence[i+(n-1)]))\n",
        "  return ngrams"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alWBnegIUUCf"
      },
      "source": [
        "Define the XhosaTextTrainDataset and XhosaTextEvalDataset classes as a subclasses of PyTorch's Dataset class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HTdpgTHEVmI"
      },
      "source": [
        "class XhosaTextTrainDataset(Dataset):\n",
        "  def __init__(self, dataset_url, ngram_size):\n",
        "    self.sentences = prepare_train_dataset(load_dataset(dataset_url))\n",
        "    self.word_index = get_word_index(self.sentences)\n",
        "    self.vocab = get_vocab(self.word_index)\n",
        "    self.ngrams = get_ngrams(self.sentences, ngram_size)\n",
        "    self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.ngrams)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    context = []\n",
        "    for word in self.ngrams[idx][0]:\n",
        "      context.append(self.word_index[word])\n",
        "    target = self.word_index[self.ngrams[idx][1]]\n",
        "    return torch.tensor(context, dtype=torch.long, device=torch.device(self.device)), torch.tensor(target, dtype=torch.long, device=torch.device(self.device))\n",
        "\n",
        "  def vocab_size(self):\n",
        "    return len(self.vocab)\n",
        "\n",
        "  def get_word_index(self):\n",
        "    return self.word_index"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNl8CFjcfRJI"
      },
      "source": [
        "class XhosaTextEvalDataset(Dataset):\n",
        "  def __init__(self, dataset_url, ngram_size, train_dataset):\n",
        "    self.word_index = train_dataset.get_word_index()\n",
        "    self.sentences = prepare_eval_dataset(load_dataset(dataset_url), self.word_index)\n",
        "    self.vocab = get_vocab(self.word_index)\n",
        "    self.ngrams = get_ngrams(self.sentences, ngram_size)\n",
        "    self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.ngrams)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    context = []\n",
        "    for word in self.ngrams[idx][0]:\n",
        "      context.append(self.word_index[word])\n",
        "    target = self.word_index[self.ngrams[idx][1]]\n",
        "    return torch.tensor(context, dtype=torch.long, device=torch.device(self.device)), torch.tensor(target, dtype=torch.long, device=torch.device(self.device))"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScHkzkMWUhol"
      },
      "source": [
        "Download and process all three isiXhosa datasets (training, validation and testing) in preparation for training and evaluating the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eflxhEc8EcwB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0d5ca42-6146-4765-a445-adc0212c9550"
      },
      "source": [
        "# Training dataset\n",
        "train_dataset = XhosaTextTrainDataset(\n",
        "  dataset_url=train_url, \n",
        "  ngram_size=NGRAM_SIZE\n",
        ")\n",
        "print(f\"Training Dataset: {train_dataset.__len__()} ngrams.\")\n",
        "\n",
        "# Validation dataset\n",
        "valid_dataset = XhosaTextEvalDataset(\n",
        "  dataset_url=valid_url, \n",
        "  ngram_size=NGRAM_SIZE,\n",
        "  train_dataset=train_dataset\n",
        ")\n",
        "print(f\"Validation Dataset: {valid_dataset.__len__()} ngrams.\")\n",
        "\n",
        "# Testing dataset\n",
        "test_dataset = XhosaTextEvalDataset(\n",
        "  dataset_url=test_url, \n",
        "  ngram_size=NGRAM_SIZE,\n",
        "  train_dataset=train_dataset\n",
        ")\n",
        "print(f\"Testing Dataset: {test_dataset.__len__()} ngrams.\")"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Dataset: 655570 ngrams.\n",
            "Validation Dataset: 36073 ngrams.\n",
            "Testing Dataset: 36599 ngrams.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvzf4S9WU8uZ"
      },
      "source": [
        "Prepare dataloaders for each dataset to efficiently shuffle and batch data for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJ7LgtjL2BNT"
      },
      "source": [
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yr98KqLGO4Ny"
      },
      "source": [
        "## Define Neural Network Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOUqsYx0WBYB"
      },
      "source": [
        "Define the structure of the custom neural network model. This model has an input embedding layer, a hidden layer and an output layer. Dropout with 0.25 probability is applied between the layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTr9iz9_jY7r"
      },
      "source": [
        "# source: https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html\n",
        "class NGramLanguageModeler(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, context_size, batch_size):\n",
        "    super(NGramLanguageModeler, self).__init__()\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.context_size = context_size\n",
        "    self.batch_size = batch_size\n",
        "    self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
        "    self.linear2 = nn.Linear(128, vocab_size)\n",
        "    self.dropout = nn.Dropout(0.25)\n",
        "  \n",
        "  def forward(self, inputs):\n",
        "    embeds = self.embeddings(inputs).view((len(inputs), self.context_size * self.embedding_dim))\n",
        "    out = self.dropout(embeds)\n",
        "    out = F.relu(self.linear1(out))\n",
        "    out = self.dropout(out)\n",
        "    out = self.linear2(out)\n",
        "    log_probs = F.log_softmax(out, dim=1)\n",
        "    return log_probs"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3846zpWjc9H"
      },
      "source": [
        "## Train Neural Network Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31Js1BdCXGBo"
      },
      "source": [
        "Finally, the neural network model is trained with the training dataset using the specified hyperparameters. The final evaluation is completed using the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9WRC3KHO9Zn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7529319d-a453-47f2-de69-e8a0b1f2ce5e"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "batch_log_interval = 200\n",
        "\n",
        "model = NGramLanguageModeler(train_dataset.vocab_size(), EMBEDDING_DIM, CONTEXT_SIZE, BATCH_SIZE).to(device)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=INITIAL_LEARNING_RATE)\n",
        "scheduler1 = ReduceLROnPlateau(optimizer, patience=1)\n",
        "scheduler2 = ExponentialLR(optimizer, gamma=0.9)\n",
        "\n",
        "for epoch_i in range(5):\n",
        "\n",
        "  print('Start Training')\n",
        "\n",
        "  # Start timing epoch\n",
        "  epoch_start_time = datetime.datetime.now()\n",
        "\n",
        "  # TRAINING PHASE\n",
        "  model.train()\n",
        "  total_batch_elapsed_time_ms = 0\n",
        "  total_batch_loss = 0\n",
        "  total_batch_ppl = 0\n",
        "\n",
        "  for batch_i, (train_context, train_target) in enumerate(train_dataloader):\n",
        "\n",
        "    # Start timing batch\n",
        "    batch_start_time = datetime.datetime.now()\n",
        "\n",
        "    # Zero gradients from old input\n",
        "    model.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    log_probs = model(train_context)\n",
        "\n",
        "    # Get loss\n",
        "    loss = loss_function(log_probs, train_target)\n",
        "    total_batch_loss += loss.item()\n",
        "\n",
        "    # Get perplexity\n",
        "    ppl = torch.exp(loss)\n",
        "    total_batch_ppl += ppl.item()\n",
        "\n",
        "    # Backward propagation\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    # Stop timing batch\n",
        "    batch_end_time = datetime.datetime.now()\n",
        "    batch_elapsed_time = (batch_end_time - batch_start_time)\n",
        "    batch_elapsed_time_ms = batch_elapsed_time.total_seconds() * 1000\n",
        "    total_batch_elapsed_time_ms += batch_elapsed_time_ms\n",
        "\n",
        "    # Log training stats in batch intervals\n",
        "    if ((batch_i+1) % batch_log_interval == 0):\n",
        "      print(\"| epoch {} | {}/ {} batches | lr {:.4f} | ms/batch {:.2f} | loss {:.2f} | ppl {:.2f}\".format(epoch_i+1, batch_i+1, len(train_dataloader), optimizer.param_groups[0]['lr'], total_batch_elapsed_time_ms/batch_log_interval, total_batch_loss/batch_log_interval, total_batch_ppl/batch_log_interval))\n",
        "      total_batch_elapsed_time_ms = 0\n",
        "      total_batch_loss = 0\n",
        "      total_batch_ppl = 0\n",
        "\n",
        "  # VALIDATION PHASE\n",
        "  model.eval()\n",
        "  total_valid_loss = 0\n",
        "  total_valid_ppl = 0\n",
        "  with torch.no_grad():\n",
        "    for batch_i, (valid_context, valid_target) in enumerate(valid_dataloader):\n",
        "\n",
        "      # Forward pass\n",
        "      log_probs = model(valid_context)\n",
        "\n",
        "      # Get loss\n",
        "      loss = loss_function(log_probs, valid_target)\n",
        "      total_valid_loss += loss.item()\n",
        "\n",
        "      # Get perplexity\n",
        "      ppl = torch.exp(loss)\n",
        "      total_valid_ppl += ppl.item()\n",
        "\n",
        "  # Stop timing epoch\n",
        "  epoch_end_time = datetime.datetime.now()\n",
        "  epoch_elapsed_time = (epoch_end_time - epoch_start_time)\n",
        "  epoch_elapsed_time_s = epoch_elapsed_time.total_seconds()\n",
        "  \n",
        "  # Update learning rate\n",
        "  scheduler1.step(total_valid_loss/len(valid_dataloader))\n",
        "  scheduler2.step()\n",
        "\n",
        "  # Log validation stats at end of each epoch\n",
        "  print(\"-------------------------------------------------------------------\")\n",
        "  print(\"| end of epoch {} | time: {:.2f}s | valid loss {:.2f} | valid ppl {:.2f}\".format(epoch_i+1, epoch_elapsed_time_s, total_valid_loss/len(valid_dataloader), total_valid_ppl/len(valid_dataloader)))\n",
        "  print(\"-------------------------------------------------------------------\")\n",
        "\n",
        "# TESTING PHASE\n",
        "model.eval()\n",
        "total_test_loss = 0\n",
        "total_test_ppl = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "  for batch_i, (test_context, test_target) in enumerate(test_dataloader):\n",
        "\n",
        "    # Forward pass\n",
        "    log_probs = model(test_context)\n",
        "\n",
        "    # Get loss\n",
        "    loss = loss_function(log_probs, test_target)\n",
        "    total_test_loss += loss.item()\n",
        "\n",
        "    # Get perplexity\n",
        "    ppl = torch.exp(loss)\n",
        "    total_test_ppl += ppl.item()\n",
        "\n",
        "# Log testing stats at end of training\n",
        "print(\"=========================================================================================\")\n",
        "print(\"| End of training | test loss {:.2f} | test ppl {:.2f}\".format(total_test_loss/len(test_dataloader), total_test_ppl/len(test_dataloader)))\n",
        "print(\"=========================================================================================\")"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start Training\n",
            "| epoch 1 | 200/ 10244 batches | lr 0.1000 | ms/batch 2.19 | loss 10.63 | ppl 43091.37\n",
            "| epoch 1 | 400/ 10244 batches | lr 0.1000 | ms/batch 2.10 | loss 10.31 | ppl 31248.50\n",
            "| epoch 1 | 600/ 10244 batches | lr 0.1000 | ms/batch 2.04 | loss 10.14 | ppl 26276.74\n",
            "| epoch 1 | 800/ 10244 batches | lr 0.1000 | ms/batch 2.04 | loss 9.98 | ppl 22621.13\n",
            "| epoch 1 | 1000/ 10244 batches | lr 0.1000 | ms/batch 2.04 | loss 9.88 | ppl 20429.69\n",
            "| epoch 1 | 1200/ 10244 batches | lr 0.1000 | ms/batch 2.03 | loss 9.79 | ppl 18733.42\n",
            "| epoch 1 | 1400/ 10244 batches | lr 0.1000 | ms/batch 2.05 | loss 9.69 | ppl 16897.98\n",
            "| epoch 1 | 1600/ 10244 batches | lr 0.1000 | ms/batch 2.04 | loss 9.59 | ppl 15586.08\n",
            "| epoch 1 | 1800/ 10244 batches | lr 0.1000 | ms/batch 2.06 | loss 9.50 | ppl 14022.23\n",
            "| epoch 1 | 2000/ 10244 batches | lr 0.1000 | ms/batch 2.04 | loss 9.45 | ppl 13596.24\n",
            "| epoch 1 | 2200/ 10244 batches | lr 0.1000 | ms/batch 2.04 | loss 9.38 | ppl 12487.71\n",
            "| epoch 1 | 2400/ 10244 batches | lr 0.1000 | ms/batch 2.03 | loss 9.32 | ppl 11873.90\n",
            "| epoch 1 | 2600/ 10244 batches | lr 0.1000 | ms/batch 2.07 | loss 9.32 | ppl 11906.32\n",
            "| epoch 1 | 2800/ 10244 batches | lr 0.1000 | ms/batch 2.09 | loss 9.30 | ppl 11672.20\n",
            "| epoch 1 | 3000/ 10244 batches | lr 0.1000 | ms/batch 2.09 | loss 9.22 | ppl 10700.12\n",
            "| epoch 1 | 3200/ 10244 batches | lr 0.1000 | ms/batch 2.07 | loss 9.16 | ppl 10062.33\n",
            "| epoch 1 | 3400/ 10244 batches | lr 0.1000 | ms/batch 2.06 | loss 9.23 | ppl 10834.70\n",
            "| epoch 1 | 3600/ 10244 batches | lr 0.1000 | ms/batch 2.04 | loss 9.17 | ppl 10173.68\n",
            "| epoch 1 | 3800/ 10244 batches | lr 0.1000 | ms/batch 2.05 | loss 9.22 | ppl 10799.31\n",
            "| epoch 1 | 4000/ 10244 batches | lr 0.1000 | ms/batch 2.04 | loss 9.16 | ppl 10214.71\n",
            "| epoch 1 | 4200/ 10244 batches | lr 0.1000 | ms/batch 2.07 | loss 9.18 | ppl 10438.02\n",
            "| epoch 1 | 4400/ 10244 batches | lr 0.1000 | ms/batch 2.03 | loss 9.07 | ppl 9248.22\n",
            "| epoch 1 | 4600/ 10244 batches | lr 0.1000 | ms/batch 2.07 | loss 9.07 | ppl 9301.23\n",
            "| epoch 1 | 4800/ 10244 batches | lr 0.1000 | ms/batch 2.03 | loss 9.06 | ppl 9263.46\n",
            "| epoch 1 | 5000/ 10244 batches | lr 0.1000 | ms/batch 2.04 | loss 9.08 | ppl 9270.59\n",
            "| epoch 1 | 5200/ 10244 batches | lr 0.1000 | ms/batch 2.07 | loss 9.06 | ppl 9160.51\n",
            "| epoch 1 | 5400/ 10244 batches | lr 0.1000 | ms/batch 2.04 | loss 9.02 | ppl 8813.52\n",
            "| epoch 1 | 5600/ 10244 batches | lr 0.1000 | ms/batch 2.04 | loss 9.09 | ppl 9484.57\n",
            "| epoch 1 | 5800/ 10244 batches | lr 0.1000 | ms/batch 2.07 | loss 9.04 | ppl 8956.58\n",
            "| epoch 1 | 6000/ 10244 batches | lr 0.1000 | ms/batch 2.08 | loss 9.00 | ppl 8572.99\n",
            "| epoch 1 | 6200/ 10244 batches | lr 0.1000 | ms/batch 2.04 | loss 9.02 | ppl 8768.65\n",
            "| epoch 1 | 6400/ 10244 batches | lr 0.1000 | ms/batch 2.04 | loss 9.02 | ppl 8843.11\n",
            "| epoch 1 | 6600/ 10244 batches | lr 0.1000 | ms/batch 2.01 | loss 9.00 | ppl 8612.45\n",
            "| epoch 1 | 6800/ 10244 batches | lr 0.1000 | ms/batch 2.02 | loss 8.98 | ppl 8563.47\n",
            "| epoch 1 | 7000/ 10244 batches | lr 0.1000 | ms/batch 2.03 | loss 9.01 | ppl 8622.34\n",
            "| epoch 1 | 7200/ 10244 batches | lr 0.1000 | ms/batch 2.04 | loss 9.00 | ppl 8503.10\n",
            "| epoch 1 | 7400/ 10244 batches | lr 0.1000 | ms/batch 2.03 | loss 8.96 | ppl 8281.93\n",
            "| epoch 1 | 7600/ 10244 batches | lr 0.1000 | ms/batch 2.06 | loss 8.99 | ppl 8480.22\n",
            "| epoch 1 | 7800/ 10244 batches | lr 0.1000 | ms/batch 2.03 | loss 8.97 | ppl 8334.16\n",
            "| epoch 1 | 8000/ 10244 batches | lr 0.1000 | ms/batch 2.09 | loss 8.99 | ppl 8533.16\n",
            "| epoch 1 | 8200/ 10244 batches | lr 0.1000 | ms/batch 2.07 | loss 9.00 | ppl 8598.65\n",
            "| epoch 1 | 8400/ 10244 batches | lr 0.1000 | ms/batch 2.05 | loss 8.98 | ppl 8593.04\n",
            "| epoch 1 | 8600/ 10244 batches | lr 0.1000 | ms/batch 2.05 | loss 8.95 | ppl 8102.54\n",
            "| epoch 1 | 8800/ 10244 batches | lr 0.1000 | ms/batch 2.03 | loss 8.97 | ppl 8337.41\n",
            "| epoch 1 | 9000/ 10244 batches | lr 0.1000 | ms/batch 2.06 | loss 8.97 | ppl 8377.27\n",
            "| epoch 1 | 9200/ 10244 batches | lr 0.1000 | ms/batch 2.01 | loss 8.97 | ppl 8369.39\n",
            "| epoch 1 | 9400/ 10244 batches | lr 0.1000 | ms/batch 2.05 | loss 8.94 | ppl 8120.29\n",
            "| epoch 1 | 9600/ 10244 batches | lr 0.1000 | ms/batch 2.03 | loss 8.90 | ppl 7793.10\n",
            "| epoch 1 | 9800/ 10244 batches | lr 0.1000 | ms/batch 2.04 | loss 8.93 | ppl 8032.87\n",
            "| epoch 1 | 10000/ 10244 batches | lr 0.1000 | ms/batch 2.05 | loss 8.91 | ppl 7877.38\n",
            "| epoch 1 | 10200/ 10244 batches | lr 0.1000 | ms/batch 2.06 | loss 8.95 | ppl 8279.14\n",
            "-------------------------------------------------------------------\n",
            "| end of epoch 1 | time: 67.72s | valid loss 8.11 | valid ppl 3592.30\n",
            "-------------------------------------------------------------------\n",
            "Start Training\n",
            "| epoch 2 | 200/ 10244 batches | lr 0.0900 | ms/batch 2.03 | loss 8.86 | ppl 7411.21\n",
            "| epoch 2 | 400/ 10244 batches | lr 0.0900 | ms/batch 2.06 | loss 8.88 | ppl 7575.63\n",
            "| epoch 2 | 600/ 10244 batches | lr 0.0900 | ms/batch 2.01 | loss 8.88 | ppl 7532.97\n",
            "| epoch 2 | 800/ 10244 batches | lr 0.0900 | ms/batch 2.05 | loss 8.87 | ppl 7496.92\n",
            "| epoch 2 | 1000/ 10244 batches | lr 0.0900 | ms/batch 2.04 | loss 8.86 | ppl 7500.25\n",
            "| epoch 2 | 1200/ 10244 batches | lr 0.0900 | ms/batch 2.04 | loss 8.85 | ppl 7516.89\n",
            "| epoch 2 | 1400/ 10244 batches | lr 0.0900 | ms/batch 2.04 | loss 8.87 | ppl 7631.23\n",
            "| epoch 2 | 1600/ 10244 batches | lr 0.0900 | ms/batch 2.04 | loss 8.92 | ppl 7948.07\n",
            "| epoch 2 | 1800/ 10244 batches | lr 0.0900 | ms/batch 2.03 | loss 8.88 | ppl 7657.03\n",
            "| epoch 2 | 2000/ 10244 batches | lr 0.0900 | ms/batch 2.02 | loss 8.84 | ppl 7305.41\n",
            "| epoch 2 | 2200/ 10244 batches | lr 0.0900 | ms/batch 2.04 | loss 8.89 | ppl 7702.38\n",
            "| epoch 2 | 2400/ 10244 batches | lr 0.0900 | ms/batch 2.04 | loss 8.91 | ppl 7831.12\n",
            "| epoch 2 | 2600/ 10244 batches | lr 0.0900 | ms/batch 2.02 | loss 8.89 | ppl 7663.04\n",
            "| epoch 2 | 2800/ 10244 batches | lr 0.0900 | ms/batch 2.05 | loss 8.85 | ppl 7407.63\n",
            "| epoch 2 | 3000/ 10244 batches | lr 0.0900 | ms/batch 2.05 | loss 8.85 | ppl 7428.91\n",
            "| epoch 2 | 3200/ 10244 batches | lr 0.0900 | ms/batch 2.05 | loss 8.84 | ppl 7401.25\n",
            "| epoch 2 | 3400/ 10244 batches | lr 0.0900 | ms/batch 2.06 | loss 8.81 | ppl 7081.93\n",
            "| epoch 2 | 3600/ 10244 batches | lr 0.0900 | ms/batch 2.04 | loss 8.88 | ppl 7562.19\n",
            "| epoch 2 | 3800/ 10244 batches | lr 0.0900 | ms/batch 2.06 | loss 8.88 | ppl 7610.50\n",
            "| epoch 2 | 4000/ 10244 batches | lr 0.0900 | ms/batch 2.04 | loss 8.86 | ppl 7528.69\n",
            "| epoch 2 | 4200/ 10244 batches | lr 0.0900 | ms/batch 2.02 | loss 8.79 | ppl 7000.37\n",
            "| epoch 2 | 4400/ 10244 batches | lr 0.0900 | ms/batch 2.03 | loss 8.87 | ppl 7583.10\n",
            "| epoch 2 | 4600/ 10244 batches | lr 0.0900 | ms/batch 2.04 | loss 8.81 | ppl 7050.96\n",
            "| epoch 2 | 4800/ 10244 batches | lr 0.0900 | ms/batch 2.05 | loss 8.84 | ppl 7321.67\n",
            "| epoch 2 | 5000/ 10244 batches | lr 0.0900 | ms/batch 2.03 | loss 8.82 | ppl 7286.08\n",
            "| epoch 2 | 5200/ 10244 batches | lr 0.0900 | ms/batch 2.04 | loss 8.83 | ppl 7226.87\n",
            "| epoch 2 | 5400/ 10244 batches | lr 0.0900 | ms/batch 2.05 | loss 8.84 | ppl 7334.33\n",
            "| epoch 2 | 5600/ 10244 batches | lr 0.0900 | ms/batch 2.03 | loss 8.84 | ppl 7349.55\n",
            "| epoch 2 | 5800/ 10244 batches | lr 0.0900 | ms/batch 2.05 | loss 8.82 | ppl 7214.24\n",
            "| epoch 2 | 6000/ 10244 batches | lr 0.0900 | ms/batch 2.04 | loss 8.81 | ppl 7072.82\n",
            "| epoch 2 | 6200/ 10244 batches | lr 0.0900 | ms/batch 2.05 | loss 8.88 | ppl 7557.47\n",
            "| epoch 2 | 6400/ 10244 batches | lr 0.0900 | ms/batch 2.02 | loss 8.84 | ppl 7263.04\n",
            "| epoch 2 | 6600/ 10244 batches | lr 0.0900 | ms/batch 2.05 | loss 8.85 | ppl 7405.36\n",
            "| epoch 2 | 6800/ 10244 batches | lr 0.0900 | ms/batch 2.04 | loss 8.80 | ppl 7078.77\n",
            "| epoch 2 | 7000/ 10244 batches | lr 0.0900 | ms/batch 2.06 | loss 8.82 | ppl 7330.53\n",
            "| epoch 2 | 7200/ 10244 batches | lr 0.0900 | ms/batch 2.05 | loss 8.81 | ppl 7132.62\n",
            "| epoch 2 | 7400/ 10244 batches | lr 0.0900 | ms/batch 2.02 | loss 8.81 | ppl 7087.35\n",
            "| epoch 2 | 7600/ 10244 batches | lr 0.0900 | ms/batch 2.05 | loss 8.80 | ppl 7069.57\n",
            "| epoch 2 | 7800/ 10244 batches | lr 0.0900 | ms/batch 2.10 | loss 8.80 | ppl 7068.90\n",
            "| epoch 2 | 8000/ 10244 batches | lr 0.0900 | ms/batch 2.06 | loss 8.87 | ppl 7576.10\n",
            "| epoch 2 | 8200/ 10244 batches | lr 0.0900 | ms/batch 2.09 | loss 8.73 | ppl 6598.71\n",
            "| epoch 2 | 8400/ 10244 batches | lr 0.0900 | ms/batch 2.08 | loss 8.76 | ppl 6746.70\n",
            "| epoch 2 | 8600/ 10244 batches | lr 0.0900 | ms/batch 2.06 | loss 8.81 | ppl 7125.57\n",
            "| epoch 2 | 8800/ 10244 batches | lr 0.0900 | ms/batch 2.08 | loss 8.79 | ppl 7013.53\n",
            "| epoch 2 | 9000/ 10244 batches | lr 0.0900 | ms/batch 2.10 | loss 8.81 | ppl 7098.35\n",
            "| epoch 2 | 9200/ 10244 batches | lr 0.0900 | ms/batch 2.07 | loss 8.81 | ppl 7137.37\n",
            "| epoch 2 | 9400/ 10244 batches | lr 0.0900 | ms/batch 2.10 | loss 8.81 | ppl 7085.56\n",
            "| epoch 2 | 9600/ 10244 batches | lr 0.0900 | ms/batch 2.07 | loss 8.80 | ppl 6992.21\n",
            "| epoch 2 | 9800/ 10244 batches | lr 0.0900 | ms/batch 2.05 | loss 8.80 | ppl 7026.76\n",
            "| epoch 2 | 10000/ 10244 batches | lr 0.0900 | ms/batch 2.03 | loss 8.85 | ppl 7427.32\n",
            "| epoch 2 | 10200/ 10244 batches | lr 0.0900 | ms/batch 2.02 | loss 8.87 | ppl 7627.07\n",
            "-------------------------------------------------------------------\n",
            "| end of epoch 2 | time: 67.57s | valid loss 8.09 | valid ppl 3536.00\n",
            "-------------------------------------------------------------------\n",
            "Start Training\n",
            "| epoch 3 | 200/ 10244 batches | lr 0.0810 | ms/batch 2.02 | loss 8.77 | ppl 6854.92\n",
            "| epoch 3 | 400/ 10244 batches | lr 0.0810 | ms/batch 2.04 | loss 8.73 | ppl 6687.69\n",
            "| epoch 3 | 600/ 10244 batches | lr 0.0810 | ms/batch 2.05 | loss 8.75 | ppl 6735.51\n",
            "| epoch 3 | 800/ 10244 batches | lr 0.0810 | ms/batch 2.04 | loss 8.77 | ppl 6917.12\n",
            "| epoch 3 | 1000/ 10244 batches | lr 0.0810 | ms/batch 2.04 | loss 8.75 | ppl 6687.36\n",
            "| epoch 3 | 1200/ 10244 batches | lr 0.0810 | ms/batch 2.02 | loss 8.74 | ppl 6628.33\n",
            "| epoch 3 | 1400/ 10244 batches | lr 0.0810 | ms/batch 2.06 | loss 8.76 | ppl 6791.38\n",
            "| epoch 3 | 1600/ 10244 batches | lr 0.0810 | ms/batch 2.04 | loss 8.71 | ppl 6465.40\n",
            "| epoch 3 | 1800/ 10244 batches | lr 0.0810 | ms/batch 2.07 | loss 8.72 | ppl 6545.35\n",
            "| epoch 3 | 2000/ 10244 batches | lr 0.0810 | ms/batch 2.02 | loss 8.73 | ppl 6508.71\n",
            "| epoch 3 | 2200/ 10244 batches | lr 0.0810 | ms/batch 2.02 | loss 8.70 | ppl 6380.35\n",
            "| epoch 3 | 2400/ 10244 batches | lr 0.0810 | ms/batch 2.05 | loss 8.76 | ppl 6756.56\n",
            "| epoch 3 | 2600/ 10244 batches | lr 0.0810 | ms/batch 2.03 | loss 8.71 | ppl 6449.09\n",
            "| epoch 3 | 2800/ 10244 batches | lr 0.0810 | ms/batch 2.05 | loss 8.75 | ppl 6724.08\n",
            "| epoch 3 | 3000/ 10244 batches | lr 0.0810 | ms/batch 2.03 | loss 8.68 | ppl 6307.10\n",
            "| epoch 3 | 3200/ 10244 batches | lr 0.0810 | ms/batch 2.04 | loss 8.74 | ppl 6616.64\n",
            "| epoch 3 | 3400/ 10244 batches | lr 0.0810 | ms/batch 2.06 | loss 8.76 | ppl 6788.57\n",
            "| epoch 3 | 3600/ 10244 batches | lr 0.0810 | ms/batch 2.02 | loss 8.70 | ppl 6425.67\n",
            "| epoch 3 | 3800/ 10244 batches | lr 0.0810 | ms/batch 2.05 | loss 8.74 | ppl 6752.63\n",
            "| epoch 3 | 4000/ 10244 batches | lr 0.0810 | ms/batch 2.00 | loss 8.67 | ppl 6197.03\n",
            "| epoch 3 | 4200/ 10244 batches | lr 0.0810 | ms/batch 2.04 | loss 8.72 | ppl 6510.42\n",
            "| epoch 3 | 4400/ 10244 batches | lr 0.0810 | ms/batch 2.03 | loss 8.69 | ppl 6358.34\n",
            "| epoch 3 | 4600/ 10244 batches | lr 0.0810 | ms/batch 2.03 | loss 8.73 | ppl 6636.87\n",
            "| epoch 3 | 4800/ 10244 batches | lr 0.0810 | ms/batch 2.07 | loss 8.67 | ppl 6197.43\n",
            "| epoch 3 | 5000/ 10244 batches | lr 0.0810 | ms/batch 2.02 | loss 8.71 | ppl 6393.03\n",
            "| epoch 3 | 5200/ 10244 batches | lr 0.0810 | ms/batch 2.00 | loss 8.74 | ppl 6659.96\n",
            "| epoch 3 | 5400/ 10244 batches | lr 0.0810 | ms/batch 2.03 | loss 8.72 | ppl 6461.34\n",
            "| epoch 3 | 5600/ 10244 batches | lr 0.0810 | ms/batch 2.03 | loss 8.67 | ppl 6217.40\n",
            "| epoch 3 | 5800/ 10244 batches | lr 0.0810 | ms/batch 2.05 | loss 8.72 | ppl 6463.95\n",
            "| epoch 3 | 6000/ 10244 batches | lr 0.0810 | ms/batch 2.05 | loss 8.72 | ppl 6550.97\n",
            "| epoch 3 | 6200/ 10244 batches | lr 0.0810 | ms/batch 2.03 | loss 8.74 | ppl 6643.29\n",
            "| epoch 3 | 6400/ 10244 batches | lr 0.0810 | ms/batch 2.02 | loss 8.71 | ppl 6386.94\n",
            "| epoch 3 | 6600/ 10244 batches | lr 0.0810 | ms/batch 2.08 | loss 8.70 | ppl 6443.93\n",
            "| epoch 3 | 6800/ 10244 batches | lr 0.0810 | ms/batch 2.05 | loss 8.70 | ppl 6338.45\n",
            "| epoch 3 | 7000/ 10244 batches | lr 0.0810 | ms/batch 2.03 | loss 8.69 | ppl 6323.97\n",
            "| epoch 3 | 7200/ 10244 batches | lr 0.0810 | ms/batch 2.04 | loss 8.71 | ppl 6473.40\n",
            "| epoch 3 | 7400/ 10244 batches | lr 0.0810 | ms/batch 2.02 | loss 8.76 | ppl 6768.34\n",
            "| epoch 3 | 7600/ 10244 batches | lr 0.0810 | ms/batch 2.08 | loss 8.77 | ppl 6912.02\n",
            "| epoch 3 | 7800/ 10244 batches | lr 0.0810 | ms/batch 2.03 | loss 8.73 | ppl 6595.60\n",
            "| epoch 3 | 8000/ 10244 batches | lr 0.0810 | ms/batch 2.05 | loss 8.67 | ppl 6170.08\n",
            "| epoch 3 | 8200/ 10244 batches | lr 0.0810 | ms/batch 2.03 | loss 8.70 | ppl 6377.85\n",
            "| epoch 3 | 8400/ 10244 batches | lr 0.0810 | ms/batch 2.03 | loss 8.72 | ppl 6492.33\n",
            "| epoch 3 | 8600/ 10244 batches | lr 0.0810 | ms/batch 2.07 | loss 8.74 | ppl 6582.69\n",
            "| epoch 3 | 8800/ 10244 batches | lr 0.0810 | ms/batch 2.04 | loss 8.71 | ppl 6424.53\n",
            "| epoch 3 | 9000/ 10244 batches | lr 0.0810 | ms/batch 2.05 | loss 8.73 | ppl 6647.31\n",
            "| epoch 3 | 9200/ 10244 batches | lr 0.0810 | ms/batch 2.06 | loss 8.69 | ppl 6429.02\n",
            "| epoch 3 | 9400/ 10244 batches | lr 0.0810 | ms/batch 2.03 | loss 8.69 | ppl 6231.69\n",
            "| epoch 3 | 9600/ 10244 batches | lr 0.0810 | ms/batch 2.06 | loss 8.71 | ppl 6427.66\n",
            "| epoch 3 | 9800/ 10244 batches | lr 0.0810 | ms/batch 2.08 | loss 8.74 | ppl 6616.39\n",
            "| epoch 3 | 10000/ 10244 batches | lr 0.0810 | ms/batch 2.01 | loss 8.68 | ppl 6330.62\n",
            "| epoch 3 | 10200/ 10244 batches | lr 0.0810 | ms/batch 2.02 | loss 8.69 | ppl 6339.71\n",
            "-------------------------------------------------------------------\n",
            "| end of epoch 3 | time: 67.41s | valid loss 8.08 | valid ppl 3492.58\n",
            "-------------------------------------------------------------------\n",
            "Start Training\n",
            "| epoch 4 | 200/ 10244 batches | lr 0.0729 | ms/batch 2.06 | loss 8.61 | ppl 5844.82\n",
            "| epoch 4 | 400/ 10244 batches | lr 0.0729 | ms/batch 2.05 | loss 8.61 | ppl 5819.52\n",
            "| epoch 4 | 600/ 10244 batches | lr 0.0729 | ms/batch 2.04 | loss 8.67 | ppl 6213.02\n",
            "| epoch 4 | 800/ 10244 batches | lr 0.0729 | ms/batch 2.09 | loss 8.64 | ppl 6038.69\n",
            "| epoch 4 | 1000/ 10244 batches | lr 0.0729 | ms/batch 2.07 | loss 8.58 | ppl 5670.42\n",
            "| epoch 4 | 1200/ 10244 batches | lr 0.0729 | ms/batch 2.04 | loss 8.61 | ppl 5852.19\n",
            "| epoch 4 | 1400/ 10244 batches | lr 0.0729 | ms/batch 2.04 | loss 8.63 | ppl 5982.77\n",
            "| epoch 4 | 1600/ 10244 batches | lr 0.0729 | ms/batch 2.03 | loss 8.61 | ppl 5875.23\n",
            "| epoch 4 | 1800/ 10244 batches | lr 0.0729 | ms/batch 2.04 | loss 8.63 | ppl 5945.69\n",
            "| epoch 4 | 2000/ 10244 batches | lr 0.0729 | ms/batch 2.04 | loss 8.62 | ppl 5931.79\n",
            "| epoch 4 | 2200/ 10244 batches | lr 0.0729 | ms/batch 2.06 | loss 8.60 | ppl 5861.43\n",
            "| epoch 4 | 2400/ 10244 batches | lr 0.0729 | ms/batch 2.03 | loss 8.61 | ppl 5865.53\n",
            "| epoch 4 | 2600/ 10244 batches | lr 0.0729 | ms/batch 2.04 | loss 8.64 | ppl 6002.92\n",
            "| epoch 4 | 2800/ 10244 batches | lr 0.0729 | ms/batch 2.09 | loss 8.57 | ppl 5588.29\n",
            "| epoch 4 | 3000/ 10244 batches | lr 0.0729 | ms/batch 2.04 | loss 8.61 | ppl 5838.16\n",
            "| epoch 4 | 3200/ 10244 batches | lr 0.0729 | ms/batch 2.08 | loss 8.62 | ppl 5860.71\n",
            "| epoch 4 | 3400/ 10244 batches | lr 0.0729 | ms/batch 2.03 | loss 8.64 | ppl 5966.81\n",
            "| epoch 4 | 3600/ 10244 batches | lr 0.0729 | ms/batch 2.04 | loss 8.62 | ppl 5966.90\n",
            "| epoch 4 | 3800/ 10244 batches | lr 0.0729 | ms/batch 2.04 | loss 8.65 | ppl 6048.25\n",
            "| epoch 4 | 4000/ 10244 batches | lr 0.0729 | ms/batch 2.06 | loss 8.67 | ppl 6195.27\n",
            "| epoch 4 | 4200/ 10244 batches | lr 0.0729 | ms/batch 2.05 | loss 8.59 | ppl 5745.33\n",
            "| epoch 4 | 4400/ 10244 batches | lr 0.0729 | ms/batch 2.03 | loss 8.63 | ppl 5944.70\n",
            "| epoch 4 | 4600/ 10244 batches | lr 0.0729 | ms/batch 2.04 | loss 8.66 | ppl 6025.81\n",
            "| epoch 4 | 4800/ 10244 batches | lr 0.0729 | ms/batch 2.03 | loss 8.60 | ppl 5742.72\n",
            "| epoch 4 | 5000/ 10244 batches | lr 0.0729 | ms/batch 2.06 | loss 8.62 | ppl 5929.52\n",
            "| epoch 4 | 5200/ 10244 batches | lr 0.0729 | ms/batch 2.03 | loss 8.61 | ppl 5787.54\n",
            "| epoch 4 | 5400/ 10244 batches | lr 0.0729 | ms/batch 2.09 | loss 8.63 | ppl 5963.00\n",
            "| epoch 4 | 5600/ 10244 batches | lr 0.0729 | ms/batch 2.03 | loss 8.60 | ppl 5828.96\n",
            "| epoch 4 | 5800/ 10244 batches | lr 0.0729 | ms/batch 2.05 | loss 8.61 | ppl 5816.31\n",
            "| epoch 4 | 6000/ 10244 batches | lr 0.0729 | ms/batch 2.01 | loss 8.61 | ppl 5835.78\n",
            "| epoch 4 | 6200/ 10244 batches | lr 0.0729 | ms/batch 2.06 | loss 8.60 | ppl 5753.90\n",
            "| epoch 4 | 6400/ 10244 batches | lr 0.0729 | ms/batch 2.05 | loss 8.63 | ppl 5967.62\n",
            "| epoch 4 | 6600/ 10244 batches | lr 0.0729 | ms/batch 2.02 | loss 8.62 | ppl 5895.55\n",
            "| epoch 4 | 6800/ 10244 batches | lr 0.0729 | ms/batch 2.05 | loss 8.67 | ppl 6204.81\n",
            "| epoch 4 | 7000/ 10244 batches | lr 0.0729 | ms/batch 2.04 | loss 8.61 | ppl 5820.76\n",
            "| epoch 4 | 7200/ 10244 batches | lr 0.0729 | ms/batch 2.03 | loss 8.62 | ppl 5905.93\n",
            "| epoch 4 | 7400/ 10244 batches | lr 0.0729 | ms/batch 2.06 | loss 8.63 | ppl 5987.39\n",
            "| epoch 4 | 7600/ 10244 batches | lr 0.0729 | ms/batch 2.07 | loss 8.63 | ppl 5963.86\n",
            "| epoch 4 | 7800/ 10244 batches | lr 0.0729 | ms/batch 2.06 | loss 8.64 | ppl 6065.07\n",
            "| epoch 4 | 8000/ 10244 batches | lr 0.0729 | ms/batch 2.02 | loss 8.62 | ppl 5883.28\n",
            "| epoch 4 | 8200/ 10244 batches | lr 0.0729 | ms/batch 2.06 | loss 8.62 | ppl 5809.08\n",
            "| epoch 4 | 8400/ 10244 batches | lr 0.0729 | ms/batch 2.09 | loss 8.61 | ppl 5783.90\n",
            "| epoch 4 | 8600/ 10244 batches | lr 0.0729 | ms/batch 2.06 | loss 8.59 | ppl 5749.96\n",
            "| epoch 4 | 8800/ 10244 batches | lr 0.0729 | ms/batch 2.06 | loss 8.63 | ppl 5936.52\n",
            "| epoch 4 | 9000/ 10244 batches | lr 0.0729 | ms/batch 2.03 | loss 8.60 | ppl 5791.61\n",
            "| epoch 4 | 9200/ 10244 batches | lr 0.0729 | ms/batch 2.03 | loss 8.61 | ppl 5805.72\n",
            "| epoch 4 | 9400/ 10244 batches | lr 0.0729 | ms/batch 2.01 | loss 8.61 | ppl 5873.37\n",
            "| epoch 4 | 9600/ 10244 batches | lr 0.0729 | ms/batch 2.03 | loss 8.61 | ppl 5870.08\n",
            "| epoch 4 | 9800/ 10244 batches | lr 0.0729 | ms/batch 2.03 | loss 8.60 | ppl 5731.32\n",
            "| epoch 4 | 10000/ 10244 batches | lr 0.0729 | ms/batch 2.06 | loss 8.62 | ppl 5822.01\n",
            "| epoch 4 | 10200/ 10244 batches | lr 0.0729 | ms/batch 2.07 | loss 8.60 | ppl 5738.08\n",
            "-------------------------------------------------------------------\n",
            "| end of epoch 4 | time: 67.56s | valid loss 8.10 | valid ppl 3530.92\n",
            "-------------------------------------------------------------------\n",
            "Start Training\n",
            "| epoch 5 | 200/ 10244 batches | lr 0.0656 | ms/batch 2.07 | loss 8.53 | ppl 5466.40\n",
            "| epoch 5 | 400/ 10244 batches | lr 0.0656 | ms/batch 2.05 | loss 8.52 | ppl 5331.60\n",
            "| epoch 5 | 600/ 10244 batches | lr 0.0656 | ms/batch 2.04 | loss 8.53 | ppl 5395.31\n",
            "| epoch 5 | 800/ 10244 batches | lr 0.0656 | ms/batch 2.03 | loss 8.53 | ppl 5404.18\n",
            "| epoch 5 | 1000/ 10244 batches | lr 0.0656 | ms/batch 2.05 | loss 8.56 | ppl 5474.18\n",
            "| epoch 5 | 1200/ 10244 batches | lr 0.0656 | ms/batch 2.05 | loss 8.56 | ppl 5535.03\n",
            "| epoch 5 | 1400/ 10244 batches | lr 0.0656 | ms/batch 2.03 | loss 8.57 | ppl 5592.45\n",
            "| epoch 5 | 1600/ 10244 batches | lr 0.0656 | ms/batch 2.04 | loss 8.50 | ppl 5204.04\n",
            "| epoch 5 | 1800/ 10244 batches | lr 0.0656 | ms/batch 2.10 | loss 8.53 | ppl 5396.93\n",
            "| epoch 5 | 2000/ 10244 batches | lr 0.0656 | ms/batch 2.07 | loss 8.58 | ppl 5703.17\n",
            "| epoch 5 | 2200/ 10244 batches | lr 0.0656 | ms/batch 2.05 | loss 8.49 | ppl 5139.96\n",
            "| epoch 5 | 2400/ 10244 batches | lr 0.0656 | ms/batch 2.04 | loss 8.48 | ppl 5135.33\n",
            "| epoch 5 | 2600/ 10244 batches | lr 0.0656 | ms/batch 2.05 | loss 8.54 | ppl 5465.99\n",
            "| epoch 5 | 2800/ 10244 batches | lr 0.0656 | ms/batch 2.06 | loss 8.50 | ppl 5241.26\n",
            "| epoch 5 | 3000/ 10244 batches | lr 0.0656 | ms/batch 2.05 | loss 8.53 | ppl 5405.73\n",
            "| epoch 5 | 3200/ 10244 batches | lr 0.0656 | ms/batch 2.07 | loss 8.55 | ppl 5589.75\n",
            "| epoch 5 | 3400/ 10244 batches | lr 0.0656 | ms/batch 2.05 | loss 8.52 | ppl 5277.21\n",
            "| epoch 5 | 3600/ 10244 batches | lr 0.0656 | ms/batch 2.03 | loss 8.52 | ppl 5344.64\n",
            "| epoch 5 | 3800/ 10244 batches | lr 0.0656 | ms/batch 2.04 | loss 8.50 | ppl 5245.62\n",
            "| epoch 5 | 4000/ 10244 batches | lr 0.0656 | ms/batch 2.05 | loss 8.51 | ppl 5301.27\n",
            "| epoch 5 | 4200/ 10244 batches | lr 0.0656 | ms/batch 2.07 | loss 8.57 | ppl 5655.33\n",
            "| epoch 5 | 4400/ 10244 batches | lr 0.0656 | ms/batch 2.02 | loss 8.54 | ppl 5417.90\n",
            "| epoch 5 | 4600/ 10244 batches | lr 0.0656 | ms/batch 2.03 | loss 8.50 | ppl 5203.93\n",
            "| epoch 5 | 4800/ 10244 batches | lr 0.0656 | ms/batch 2.04 | loss 8.53 | ppl 5389.20\n",
            "| epoch 5 | 5000/ 10244 batches | lr 0.0656 | ms/batch 2.03 | loss 8.51 | ppl 5247.02\n",
            "| epoch 5 | 5200/ 10244 batches | lr 0.0656 | ms/batch 2.02 | loss 8.56 | ppl 5569.35\n",
            "| epoch 5 | 5400/ 10244 batches | lr 0.0656 | ms/batch 2.04 | loss 8.51 | ppl 5233.05\n",
            "| epoch 5 | 5600/ 10244 batches | lr 0.0656 | ms/batch 2.05 | loss 8.53 | ppl 5338.01\n",
            "| epoch 5 | 5800/ 10244 batches | lr 0.0656 | ms/batch 2.08 | loss 8.51 | ppl 5331.42\n",
            "| epoch 5 | 6000/ 10244 batches | lr 0.0656 | ms/batch 2.06 | loss 8.50 | ppl 5315.44\n",
            "| epoch 5 | 6200/ 10244 batches | lr 0.0656 | ms/batch 2.04 | loss 8.55 | ppl 5499.06\n",
            "| epoch 5 | 6400/ 10244 batches | lr 0.0656 | ms/batch 2.04 | loss 8.55 | ppl 5516.77\n",
            "| epoch 5 | 6600/ 10244 batches | lr 0.0656 | ms/batch 2.04 | loss 8.52 | ppl 5439.92\n",
            "| epoch 5 | 6800/ 10244 batches | lr 0.0656 | ms/batch 2.02 | loss 8.54 | ppl 5420.43\n",
            "| epoch 5 | 7000/ 10244 batches | lr 0.0656 | ms/batch 2.06 | loss 8.59 | ppl 5763.04\n",
            "| epoch 5 | 7200/ 10244 batches | lr 0.0656 | ms/batch 2.06 | loss 8.49 | ppl 5111.08\n",
            "| epoch 5 | 7400/ 10244 batches | lr 0.0656 | ms/batch 2.02 | loss 8.53 | ppl 5382.48\n",
            "| epoch 5 | 7600/ 10244 batches | lr 0.0656 | ms/batch 2.07 | loss 8.48 | ppl 5114.15\n",
            "| epoch 5 | 7800/ 10244 batches | lr 0.0656 | ms/batch 2.03 | loss 8.51 | ppl 5231.13\n",
            "| epoch 5 | 8000/ 10244 batches | lr 0.0656 | ms/batch 2.06 | loss 8.49 | ppl 5200.24\n",
            "| epoch 5 | 8200/ 10244 batches | lr 0.0656 | ms/batch 2.07 | loss 8.51 | ppl 5251.36\n",
            "| epoch 5 | 8400/ 10244 batches | lr 0.0656 | ms/batch 2.06 | loss 8.51 | ppl 5277.85\n",
            "| epoch 5 | 8600/ 10244 batches | lr 0.0656 | ms/batch 2.07 | loss 8.53 | ppl 5370.93\n",
            "| epoch 5 | 8800/ 10244 batches | lr 0.0656 | ms/batch 2.07 | loss 8.57 | ppl 5622.38\n",
            "| epoch 5 | 9000/ 10244 batches | lr 0.0656 | ms/batch 2.05 | loss 8.49 | ppl 5207.37\n",
            "| epoch 5 | 9200/ 10244 batches | lr 0.0656 | ms/batch 2.05 | loss 8.50 | ppl 5203.70\n",
            "| epoch 5 | 9400/ 10244 batches | lr 0.0656 | ms/batch 2.05 | loss 8.54 | ppl 5470.87\n",
            "| epoch 5 | 9600/ 10244 batches | lr 0.0656 | ms/batch 2.03 | loss 8.58 | ppl 5614.75\n",
            "| epoch 5 | 9800/ 10244 batches | lr 0.0656 | ms/batch 2.05 | loss 8.45 | ppl 5003.22\n",
            "| epoch 5 | 10000/ 10244 batches | lr 0.0656 | ms/batch 2.02 | loss 8.51 | ppl 5287.54\n",
            "| epoch 5 | 10200/ 10244 batches | lr 0.0656 | ms/batch 2.05 | loss 8.54 | ppl 5461.67\n",
            "-------------------------------------------------------------------\n",
            "| end of epoch 5 | time: 67.56s | valid loss 8.02 | valid ppl 3294.61\n",
            "-------------------------------------------------------------------\n",
            "=========================================================================================\n",
            "| End of training | test loss 8.02 | test ppl 3313.79\n",
            "=========================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp-feedforward-nn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scotthallauer/nlp-feedforward-nn/blob/main/nlp_feedforward_nn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQJwOwG5ApUg"
      },
      "source": [
        "# NLP Assignment 2\n",
        "\n",
        "**Authors:** Scott Hallauer (HLLSCO001) and Steve Wang (WNGSHU003)\n",
        "\n",
        "**Date:** 21 June 2021"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TZir7sSW_7z"
      },
      "source": [
        "## Set-Up\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gr-77pCjAnKz"
      },
      "source": [
        "import urllib\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYZ_fg0Cg8GQ"
      },
      "source": [
        "## Prepare Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiPZG9y6g_sW"
      },
      "source": [
        "train_url = \"https://raw.githubusercontent.com/scotthallauer/nlp-feedforward-nn/main/xh-data/nchlt_text.xh.train\"\n",
        "valid_url = \"https://raw.githubusercontent.com/scotthallauer/nlp-feedforward-nn/main/xh-data/nchlt_text.xh.valid\"\n",
        "test_url = \"https://raw.githubusercontent.com/scotthallauer/nlp-feedforward-nn/main/xh-data/nchlt_text.xh.test\""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2woZxSklaDL"
      },
      "source": [
        "def load_dataset(url):\n",
        "  \"\"\"Load a raw dataset from the given URL and convert it into a list of lower-case, punctuation-cleaned sentences.\"\"\"\n",
        "  dataset = []\n",
        "  with urllib.request.urlopen(url) as data:\n",
        "    for line in data:\n",
        "      input_sentence = line.decode().split()\n",
        "      output_sentence = []\n",
        "      for input_word in input_sentence:\n",
        "        output_word = input_word.lower()\n",
        "        output_word = re.sub(r\"(\\.|\\,|\\:|\\?|\\)|\\\"|\\-|\\!|\\/|\\')+$\", \"\", output_word)\n",
        "        output_word = re.sub(r\"^(\\(|\\\"|\\-|\\/|\\')+\", \"\", output_word)\n",
        "        if len(output_word) > 0:\n",
        "          output_sentence.append(output_word)\n",
        "      dataset.append(output_sentence)\n",
        "  return dataset\n",
        "\n",
        "def prepare_dataset(dataset):\n",
        "  \"\"\"Replace all words that only occur once in the dataset with the <UNK> token.\"\"\"\n",
        "  word_counts = {}\n",
        "  for sentence in dataset:\n",
        "    for word in sentence:\n",
        "      if word not in word_counts:\n",
        "        word_counts[word] = 1\n",
        "      else:\n",
        "        word_counts[word] += 1\n",
        "  for i in range(len(dataset)):\n",
        "    for j in range(len(dataset[i])):\n",
        "      if word_counts[dataset[i][j]] == 1:\n",
        "        dataset[i][j] = \"<UNK>\"\n",
        "  return dataset\n",
        "\n",
        "def get_word_index(dataset):\n",
        "  word_index = {}\n",
        "  index = 0\n",
        "  for sentence in dataset:\n",
        "    for word in sentence:\n",
        "      if word not in word_index:\n",
        "        word_index[word] = index\n",
        "        index += 1\n",
        "  return word_index\n",
        "\n",
        "def get_word_list(word_index):\n",
        "  word_list = []\n",
        "  for i in range(len(word_index)):\n",
        "    word_list.append(\"\");\n",
        "  for word in word_index:\n",
        "    word_list[word_index[word]] = word\n",
        "  return word_list\n",
        "\n",
        "def get_n_gram_list(dataset, n):\n",
        "  n_gram_list = []\n",
        "  for sentence in dataset:\n",
        "    if len(sentence) >= n:\n",
        "      for i in range(len(sentence)-(n-1)):\n",
        "        n_gram_list.append(sentence[i:i+n])\n",
        "  return n_gram_list\n",
        "\n",
        "def get_n_gram_counts(n_gram_list):\n",
        "  n_gram_counts = {}\n",
        "  for n_gram in n_gram_list:\n",
        "    n_gram_key = repr(n_gram)\n",
        "    if n_gram_key not in n_gram_counts:\n",
        "      n_gram_counts[n_gram_key] = 1\n",
        "    else:\n",
        "      n_gram_counts[n_gram_key] += 1\n",
        "  return n_gram_counts\n",
        "\n",
        "def get_n_gram_prefix_counts(n_gram_list):\n",
        "  n_gram_prefix_counts = {}\n",
        "  for n_gram in n_gram_list:\n",
        "    n_gram_prefix = n_gram[0:-1]\n",
        "    n_gram_prefix_key = repr(n_gram_prefix)\n",
        "    if n_gram_prefix_key not in n_gram_prefix_counts:\n",
        "      n_gram_prefix_counts[n_gram_prefix_key] = 1\n",
        "    else:\n",
        "      n_gram_prefix_counts[n_gram_prefix_key] += 1\n",
        "  return n_gram_prefix_counts\n",
        "\n",
        "def get_n_gram_count(n_gram, n_gram_counts):\n",
        "  n_gram_key = repr(n_gram)\n",
        "  if n_gram_key not in n_gram_counts:\n",
        "    return 0\n",
        "  else:\n",
        "    return n_gram_counts[n_gram_key]\n",
        "\n",
        "def get_n_gram_probability(sequence_prefix, target_suffix, n_gram_counts, n_gram_prefix_counts):\n",
        "  n_gram = sequence_prefix + [target_suffix]\n",
        "  n_gram_count = get_n_gram_count(n_gram, n_gram_counts)\n",
        "  n_gram_prefix_count = get_n_gram_count(sequence_prefix, n_gram_prefix_counts)\n",
        "  if n_gram_count == 0 or n_gram_prefix_count == 0:\n",
        "    return 0\n",
        "  else:\n",
        "    return n_gram_count / n_gram_prefix_count\n",
        "\n",
        "#def get_n_gram_dist(dataset, n):\n",
        "#  \"\"\"P(w_n | w_1 w_2 ... w_n-1)\"\"\"\n",
        "#  word_index = get_word_index(dataset)\n",
        "#  word_list = get_word_list(word_index)\n",
        "#  n_gram_list = get_n_gram_list(dataset, n)\n",
        "#  n_gram_counts = get_n_gram_counts(n_gram_list)\n",
        "#  n_sub1_gram_list = get_n_gram_list(dataset, n-1)\n",
        "#  n_sub1_gram_counts = get_n_gram_counts(n_gram_list)\n",
        "#  n_gram_dist = {}\n",
        "#  for pre_sequence in n_sub1_gram_list:\n",
        "#    pre_sequence_key = repr(pre_sequence)\n",
        "#    if pre_sequence_key not in n_gram_dist:\n",
        "#      n_gram_dist[pre_sequence_key] = {}\n",
        "#      for word in word_list:\n",
        "#        n_gram = pre_sequence + [word]\n",
        "#        pre_sequence_count = get_n_gram_count(pre_sequence, n_sub1_gram_counts)\n",
        "#        n_gram_count = get_n_gram_count(n_gram, n_gram_counts)\n",
        "#        if pre_sequence_count == 0:\n",
        "#          n_gram_dist[pre_sequence_key][word] = 0\n",
        "#        else:\n",
        "#          n_gram_dist[pre_sequence_key][word] = n_gram_count / pre_sequence_count\n",
        "\n"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVlMJBuljofa"
      },
      "source": [
        "train_data = load_dataset(train_url)\n",
        "train_data = prepare_dataset(train_data)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nc5yN0NoJbPe"
      },
      "source": [
        "word_index = get_word_index(train_data)\n",
        "word_list = get_word_list(word_index)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpFojVGlMkmq",
        "outputId": "f3944a55-958f-4cef-9bb7-d013cc8122f8"
      },
      "source": [
        "print(len(word_list))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "62244\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ue6U5zfOQ8KX",
        "outputId": "da4121a3-ad65-4833-fe6c-8289d0be6610"
      },
      "source": [
        "print(train_data[70850])"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['akufuneki', '<UNK>', '<UNK>', 'axhonywe', 'kwindlela', 'yokuphosa']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_J9kg_Wevrl"
      },
      "source": [
        "n_gram_list = get_n_gram_list(train_data, 3)\n",
        "n_gram_counts = get_n_gram_counts(n_gram_list)"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsyyNlkzi3mD",
        "outputId": "6d26a564-7f01-493f-bb37-8a9f9533ed6f"
      },
      "source": [
        "print(n_gram_list[5])\n",
        "print(n_gram_counts[repr(n_gram_list[5])])"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ngocoselelo', 'njengoko', 'siqulethe']\n",
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7n9o7gnxoATN"
      },
      "source": [
        "n_gram_list = get_n_gram_list(train_data, 3)\n",
        "n_gram_counts = get_n_gram_counts(n_gram_list)\n",
        "n_gram_prefix_counts = get_n_gram_prefix_counts(n_gram_list)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05Md_jrLwb6a",
        "outputId": "21b64b1b-21bb-41a7-8590-622d3ea756cc"
      },
      "source": [
        "sequence_prefix = ['<UNK>', '<UNK>']\n",
        "target_suffix = 'axhonywe'\n",
        "print(get_n_gram_probability(sequence_prefix, target_suffix, n_gram_counts, n_gram_prefix_counts))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.00012359411692003462\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmOT3I-9xTtM"
      },
      "source": [
        "## Create MLP Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYbZUQuXxTJW"
      },
      "source": [
        "# source: https://www.oreilly.com/library/view/natural-language-processing/9781491978221/ch04.html\n",
        "# source: https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html\n",
        "# source: https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/01-basics/feedforward_neural_network/main.py#L37-L49\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MultilayerPerceptron(nn.Module):\n",
        "    def __init__(self, context_size, embedding_size, vocabulary_size):\n",
        "        super(MultilayerPerceptron, self).__init__()\n",
        "        self.fc1 = nn.Linear(context_size, embedding_size)\n",
        "        self.fc2 = nn.Linear(embedding_size, vocabulary_size)\n",
        "\n",
        "    def forward(self, x_in):\n",
        "        intermediate = F.relu(self.fc1(x_in))\n",
        "        output = self.fc2(intermediate)\n",
        "        return  F.softmax(output, dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lP76tQ69yDqi"
      },
      "source": [
        "batch_size = 3 # number of samples input at once\n",
        "input_dim = 6\n",
        "hidden_dim = 3\n",
        "output_dim = 50\n",
        "\n",
        "# Initialize model\n",
        "mlp = MultilayerPerceptron(input_dim, hidden_dim, output_dim)\n",
        "print(mlp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XJxiCnDydA5"
      },
      "source": [
        "def describe(x):\n",
        "    print(\"Type: {}\".format(x.type()))\n",
        "    print(\"Shape/size: {}\".format(x.shape))\n",
        "    print(\"Values: \\n{}\".format(x))\n",
        "\n",
        "###########################\n",
        "#Format                   #\n",
        "#Batch1: \n",
        "#[ [word1, word 2, tagert],\n",
        "#  [word1, word 2, tagert],\n",
        "#  [word1, word 2, tagert],\n",
        "#]\n",
        "\n",
        "#x = [[[0, 10, 2 ,1, 2, 4]],\n",
        "#    [[1, 11, 1 ,3, 2, 5]],\n",
        "#    [[9, 10, 6, 7, 6, 5]]]\n",
        "#x_input = torch.FloatTensor(x)\n",
        "x_input = torch.rand(batch_size, input_dim)\n",
        "describe(x_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCzqQ1mXyfDV"
      },
      "source": [
        "y_output = mlp(x_input)\n",
        "describe(y_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "It4cDh0X8pFi"
      },
      "source": [
        "Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKBrTI8S8wBI"
      },
      "source": [
        "learning_rate = 20\n",
        "\n",
        "# Use cross entroy loss as loss function\n",
        "cross_entropy_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "# Use stochastic gradient descent as optimization function\n",
        "gradient_decent_optimiser = torch.optim.SGD(mlp.parameters(), lr=learning_rate)\n",
        "\n",
        "print('Training log:')\n",
        "num_batches = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "  print('Start Training')\n",
        "  for i, (example_input, example_output) in enumerate(train_loader):\n",
        "    # Forward pass\n",
        "    output = mlp(example_input)\n",
        "    loss = cross_entropy_loss(output, example_output)\n",
        "\n",
        "    # Backward Propagation and optimisation\n",
        "    gradient_decent_optimiser.zero_grad\n",
        "    loss.backward()\n",
        "    gradient_decent_optimiser.step()\n",
        "\n",
        "    perplexity = torch.exp(loss)\n",
        "    if i == (num_batches - 1):\n",
        "      print('|epoch {}| {}/{} batches | lr {} | ms/batch {}| loss {:.2f}| ppl {:.2f}'.format(epoch+1, (i+1)*batch_size, num_batch, learning_rate, batch_time, loss.item(), perplexity))\n",
        "  # TODO: validation\n",
        "  print('-------------------------------------------------------------------')\n",
        "  print('|end of epoch {}| time: {}s| valid loss {:.2f} | valid ppl {:.2f}'.format(epoch+1, epoch_time, valid_loss, valid_ppl))\n",
        "  print('-------------------------------------------------------------------')\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
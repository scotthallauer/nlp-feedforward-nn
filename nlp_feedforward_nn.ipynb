{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp-feedforward-nn.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "aYZ_fg0Cg8GQ"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scotthallauer/nlp-feedforward-nn/blob/main/nlp_feedforward_nn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQJwOwG5ApUg"
      },
      "source": [
        "# NLP Assignment 2\n",
        "\n",
        "**Authors:** Scott Hallauer (HLLSCO001) and Steve Wang (WNGSHU003)\n",
        "\n",
        "**Date:** 21 June 2021"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TZir7sSW_7z"
      },
      "source": [
        "## Set-Up\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YACofn5oRF8W"
      },
      "source": [
        "Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gr-77pCjAnKz"
      },
      "source": [
        "import urllib\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zp-QRNj2RJaH"
      },
      "source": [
        "Set model parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXppOZ-zRL1Z"
      },
      "source": [
        "NGRAM_SIZE = 3\n",
        "EMBEDDING_DIM = 10\n",
        "CONTEXT_SIZE = 2\n",
        "LEARNING_RATE = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiPZG9y6g_sW"
      },
      "source": [
        "train_url = \"https://raw.githubusercontent.com/scotthallauer/nlp-feedforward-nn/main/xh-data/nchlt_text.xh.train\"\n",
        "valid_url = \"https://raw.githubusercontent.com/scotthallauer/nlp-feedforward-nn/main/xh-data/nchlt_text.xh.valid\"\n",
        "test_url = \"https://raw.githubusercontent.com/scotthallauer/nlp-feedforward-nn/main/xh-data/nchlt_text.xh.test\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYZ_fg0Cg8GQ"
      },
      "source": [
        "## Prepare Datasets (OLD)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2woZxSklaDL"
      },
      "source": [
        "def load_dataset(url):\n",
        "  \"\"\"Load a raw dataset from the given URL and convert it into a list of lower-case, punctuation-cleaned sentences.\"\"\"\n",
        "  dataset = []\n",
        "  with urllib.request.urlopen(url) as data:\n",
        "    for line in data:\n",
        "      input_sentence = line.decode().split()\n",
        "      output_sentence = []\n",
        "      for input_word in input_sentence:\n",
        "        output_word = input_word.lower()\n",
        "        output_word = re.sub(r\"(\\.|\\,|\\:|\\?|\\)|\\\"|\\-|\\!|\\/|\\')+$\", \"\", output_word)\n",
        "        output_word = re.sub(r\"^(\\(|\\\"|\\-|\\/|\\')+\", \"\", output_word)\n",
        "        if len(output_word) > 0:\n",
        "          output_sentence.append(output_word)\n",
        "      dataset.append(output_sentence)\n",
        "  return dataset\n",
        "\n",
        "def prepare_dataset(dataset):\n",
        "  \"\"\"Replace all words that only occur once in the dataset with the <UNK> token.\"\"\"\n",
        "  word_counts = {}\n",
        "  for sentence in dataset:\n",
        "    for word in sentence:\n",
        "      if word not in word_counts:\n",
        "        word_counts[word] = 1\n",
        "      else:\n",
        "        word_counts[word] += 1\n",
        "  for i in range(len(dataset)):\n",
        "    for j in range(len(dataset[i])):\n",
        "      if word_counts[dataset[i][j]] == 1:\n",
        "        dataset[i][j] = \"<UNK>\"\n",
        "  return dataset\n",
        "\n",
        "def get_word_index(dataset):\n",
        "  word_index = {}\n",
        "  index = 0\n",
        "  for sentence in dataset:\n",
        "    for word in sentence:\n",
        "      if word not in word_index:\n",
        "        word_index[word] = index\n",
        "        index += 1\n",
        "  return word_index\n",
        "\n",
        "def get_word_list(word_index):\n",
        "  word_list = []\n",
        "  for i in range(len(word_index)):\n",
        "    word_list.append(\"\");\n",
        "  for word in word_index:\n",
        "    word_list[word_index[word]] = word\n",
        "  return word_list\n",
        "\n",
        "def get_n_grams(dataset, n):\n",
        "  n_grams = []\n",
        "  for sentence in dataset:\n",
        "    if len(sentence) >= n:\n",
        "      for i in range(len(sentence)-(n-1)):\n",
        "        n_grams.append((sentence[i:i+(n-1)], sentence[i+(n-1)]))\n",
        "  return n_grams\n",
        "\n",
        "def get_n_gram_counts(n_grams):\n",
        "  n_gram_counts = {}\n",
        "  for context, target in n_grams:\n",
        "    n_gram_key = repr((context, target))\n",
        "    if n_gram_key not in n_gram_counts:\n",
        "      n_gram_counts[n_gram_key] = 1\n",
        "    else:\n",
        "      n_gram_counts[n_gram_key] += 1\n",
        "  return n_gram_counts\n",
        "\n",
        "def get_n_gram_context_counts(n_grams):\n",
        "  n_gram_context_counts = {}\n",
        "  for context, target in n_grams:\n",
        "    n_gram_context_key = repr(context)\n",
        "    if n_gram_context_key not in n_gram_context_counts:\n",
        "      n_gram_context_counts[n_gram_context_key] = 1\n",
        "    else:\n",
        "      n_gram_context_counts[n_gram_context_key] += 1\n",
        "  return n_gram_context_counts\n",
        "\n",
        "def get_n_gram_count(n_gram, n_gram_counts):\n",
        "  n_gram_key = repr(n_gram)\n",
        "  if n_gram_key not in n_gram_counts:\n",
        "    return 0\n",
        "  else:\n",
        "    return n_gram_counts[n_gram_key]\n",
        "\n",
        "def get_n_gram_probability(context, target, n_gram_counts, n_gram_context_counts):\n",
        "  \"\"\"P(w_n | w_1 w_2 ... w_n-1)\"\"\"\n",
        "  n_gram = (context, target)\n",
        "  n_gram_count = get_n_gram_count(n_gram, n_gram_counts)\n",
        "  n_gram_context_count = get_n_gram_count(context, n_gram_context_counts)\n",
        "  if n_gram_count == 0 or n_gram_context_count == 0:\n",
        "    return 0\n",
        "  else:\n",
        "    return n_gram_count / n_gram_context_count\n",
        "\n",
        "#def get_n_gram_dist(dataset, n):\n",
        "#  \"\"\"P(w_n | w_1 w_2 ... w_n-1)\"\"\"\n",
        "#  word_index = get_word_index(dataset)\n",
        "#  word_list = get_word_list(word_index)\n",
        "#  n_gram_list = get_n_gram_list(dataset, n)\n",
        "#  n_gram_counts = get_n_gram_counts(n_gram_list)\n",
        "#  n_sub1_gram_list = get_n_gram_list(dataset, n-1)\n",
        "#  n_sub1_gram_counts = get_n_gram_counts(n_gram_list)\n",
        "#  n_gram_dist = {}\n",
        "#  for pre_sequence in n_sub1_gram_list:\n",
        "#    pre_sequence_key = repr(pre_sequence)\n",
        "#    if pre_sequence_key not in n_gram_dist:\n",
        "#      n_gram_dist[pre_sequence_key] = {}\n",
        "#      for word in word_list:\n",
        "#        n_gram = pre_sequence + [word]\n",
        "#        pre_sequence_count = get_n_gram_count(pre_sequence, n_sub1_gram_counts)\n",
        "#        n_gram_count = get_n_gram_count(n_gram, n_gram_counts)\n",
        "#        if pre_sequence_count == 0:\n",
        "#          n_gram_dist[pre_sequence_key][word] = 0\n",
        "#        else:\n",
        "#          n_gram_dist[pre_sequence_key][word] = n_gram_count / pre_sequence_count\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9FV2j8GP-o6"
      },
      "source": [
        "Load and clean the training dataset. Then replace all words which only occur once with the \\<UNK\\> token."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVlMJBuljofa"
      },
      "source": [
        "train_data = load_dataset(train_url)\n",
        "train_data = prepare_dataset(train_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3pfofCAQll8"
      },
      "source": [
        "From the training dataset, we now extract the vocabulary and generate a dictionary with the word-to-index mappings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nc5yN0NoJbPe"
      },
      "source": [
        "word_index = get_word_index(train_data)\n",
        "vocabulary = get_word_list(word_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-fvb7IaQ2XQ"
      },
      "source": [
        "Finally, we extract all occurrences of n-grams from the training dataset and get the corresponding counts of both the full n-grams and the prefix (from 1 to n-1) n-grams."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_J9kg_Wevrl"
      },
      "source": [
        "n_grams = get_n_grams(train_data, N_GRAM_SIZE)\n",
        "n_gram_counts = get_n_gram_counts(n_grams)\n",
        "context_counts = get_n_gram_context_counts(n_grams)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sny3pWzsQYyB"
      },
      "source": [
        "All code blocks below are just for exploring the newly created representations of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpFojVGlMkmq",
        "outputId": "ecbb7ac2-8332-45f7-92d7-fd1dabca0dba"
      },
      "source": [
        "print(len(vocabulary))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "62244\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ue6U5zfOQ8KX",
        "outputId": "970df7f9-96b7-4506-88ec-aeda50cef8ef"
      },
      "source": [
        "print(train_data[70835])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['xa', '<UNK>', 'abalobi', 'ababandakanyekayo', 'neegiyeri', 'kufuneka', '<UNK>', 'ezomeleleyo', 'ezibukhali', 'kunye/okanye', 'amazembe', 'ngexesha', 'lokusebenza', 'ukusika', 'iihaki', 'ezithintelayo', 'okanye', 'imitya', 'yokuloba']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsyyNlkzi3mD",
        "outputId": "ac5e4069-1ea4-4493-b083-af52edf77c8f"
      },
      "source": [
        "print(n_grams[5])\n",
        "print(n_gram_counts[repr(n_grams[5])])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(['ngocoselelo', 'njengoko'], 'siqulethe')\n",
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05Md_jrLwb6a",
        "outputId": "47e4e482-90ff-483e-a700-47b0036ec46f"
      },
      "source": [
        "context = [\"xa\", \"<UNK>\"]\n",
        "target = \"abalobi\"\n",
        "print(get_n_gram_probability(context, target, n_gram_counts, context_counts))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0024875621890547263\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "priIoyy_EKrO"
      },
      "source": [
        "## Prepare Datasets (NEW)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K63_zdwFEOko"
      },
      "source": [
        "def load_dataset(url):\n",
        "  dataset = []\n",
        "  with urllib.request.urlopen(url) as data:\n",
        "    for line in data:\n",
        "      input_sentence = line.decode().split()\n",
        "      output_sentence = []\n",
        "      for input_word in input_sentence:\n",
        "        output_word = input_word.lower()\n",
        "        output_word = re.sub(r\"(\\.|\\,|\\:|\\?|\\)|\\\"|\\-|\\!|\\/|\\')+$\", \"\", output_word)\n",
        "        output_word = re.sub(r\"^(\\(|\\\"|\\-|\\/|\\')+\", \"\", output_word)\n",
        "        if len(output_word) > 0:\n",
        "          output_sentence.append(output_word)\n",
        "      dataset.append(output_sentence)\n",
        "  word_counts = {}\n",
        "  for sentence in dataset:\n",
        "    for word in sentence:\n",
        "      if word not in word_counts:\n",
        "        word_counts[word] = 1\n",
        "      else:\n",
        "        word_counts[word] += 1\n",
        "  for i in range(len(dataset)):\n",
        "    for j in range(len(dataset[i])):\n",
        "      if word_counts[dataset[i][j]] == 1:\n",
        "        dataset[i][j] = \"<unk>\"\n",
        "  return dataset\n",
        "\n",
        "def get_word_index(sentences):\n",
        "  word_index = {}\n",
        "  index = 0\n",
        "  for sentence in sentences:\n",
        "    for word in sentence:\n",
        "      if word not in word_index:\n",
        "        word_index[word] = index\n",
        "        index += 1\n",
        "  return word_index\n",
        "\n",
        "def get_vocab(word_index):\n",
        "  vocab = []\n",
        "  for i in range(len(word_index)):\n",
        "    vocab.append(\"\");\n",
        "  for word in word_index:\n",
        "    vocab[word_index[word]] = word\n",
        "  return vocab\n",
        "\n",
        "def get_ngrams(sentences, n):\n",
        "  ngrams = []\n",
        "  for sentence in sentences:\n",
        "    if len(sentence) >= n:\n",
        "      for i in range(len(sentence)-(n-1)):\n",
        "        ngrams.append((sentence[i:i+(n-1)], sentence[i+(n-1)]))\n",
        "  return ngrams"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HTdpgTHEVmI"
      },
      "source": [
        "class XhosaTextDataset(Dataset):\n",
        "  def __init__(self, dataset_url, ngram_size):\n",
        "    self.sentences = load_dataset(dataset_url)\n",
        "    self.word_index = get_word_index(self.sentences)\n",
        "    self.vocab = get_vocab(self.word_index)\n",
        "    self.ngrams = get_ngrams(self.sentences, ngram_size)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.ngrams)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    context = []\n",
        "    for word in self.ngrams[idx][0]:\n",
        "      context.append(self.word_index[word])\n",
        "    target = self.word_index[self.ngrams[idx][1]]\n",
        "    return torch.tensor(context), torch.tensor(target)\n",
        "\n",
        "  def vocab_size(self):\n",
        "    return len(self.vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eflxhEc8EcwB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49a9bc20-278e-43aa-fa78-ba357a266ac1"
      },
      "source": [
        "train_dataset = XhosaTextDataset(\n",
        "  dataset_url=train_url, \n",
        "  ngram_size=NGRAM_SIZE,\n",
        ")\n",
        "print(f\"Training Dataset: {train_dataset.__len__()} ngrams.\")\n",
        "\n",
        "valid_dataset = XhosaTextDataset(\n",
        "  dataset_url=valid_url, \n",
        "  ngram_size=NGRAM_SIZE,\n",
        ")\n",
        "print(f\"Validation Dataset: {valid_dataset.__len__()} ngrams.\")\n",
        "\n",
        "test_dataset = XhosaTextDataset(\n",
        "  dataset_url=test_url, \n",
        "  ngram_size=NGRAM_SIZE,\n",
        ")\n",
        "print(f\"Testing Dataset: {test_dataset.__len__()} ngrams.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Dataset: 859472 ngrams.\n",
            "Validation Dataset: 47430 ngrams.\n",
            "Testing Dataset: 47911 ngrams.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ThOPpfDHH3F",
        "outputId": "34abc542-3e05-4f41-8d9e-a2cf4bba9c4d"
      },
      "source": [
        "train_dataset.__getitem__(400)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([22, 46]), tensor(319))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJ7LgtjL2BNT"
      },
      "source": [
        "train_dataloader = DataLoader(train_dataset, batch_size=200, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6_3PMOPFYw4",
        "outputId": "fe193cce-186e-41e0-ea2e-1e0ede7b81f6"
      },
      "source": [
        "train_contexts, train_targets = next(iter(train_dataloader))\n",
        "print(f\"Contexts batch shape: {train_contexts.size()}\")\n",
        "print(f\"Targets batch shape: {train_targets.size()}\")\n",
        "context = train_contexts[0]\n",
        "target = train_targets[0]\n",
        "print(f\"Context: {context}\")\n",
        "print(f\"Target: {target}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Contexts batch shape: torch.Size([200, 2])\n",
            "Targets batch shape: torch.Size([200])\n",
            "Context: tensor([40612,    39])\n",
            "Target: 30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yr98KqLGO4Ny"
      },
      "source": [
        "## Define Neural Network Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTr9iz9_jY7r"
      },
      "source": [
        "# source: https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html\n",
        "class NGramLanguageModeler(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, context_size):\n",
        "    super(NGramLanguageModeler, self).__init__()\n",
        "    self.embeddings = nn.Embedding(train_dataset.vocab_size(), embedding_dim)\n",
        "    self.linear1 = nn.Linear((context_size * embedding_dim), 128)\n",
        "    self.linear2 = nn.Linear(128, vocab_size)\n",
        "  \n",
        "  def forward(self, inputs):\n",
        "    print(inputs.shape)\n",
        "    embeds = self.embeddings(inputs).view((1, -1))\n",
        "    print(embeds.shape)\n",
        "    out = F.relu(self.linear1(embeds))\n",
        "    out = self.linear2(out)\n",
        "    log_probs = F.log_softmax(out, dim=1)\n",
        "    return log_probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3846zpWjc9H"
      },
      "source": [
        "## Train Neural Network Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9WRC3KHO9Zn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "23ae4ef0-771e-480f-8158-4b1469108176"
      },
      "source": [
        "losses = []\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = NGramLanguageModeler(train_dataset.vocab_size(), EMBEDDING_DIM, CONTEXT_SIZE).to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "print('Training log:')\n",
        "for epoch in range(10):\n",
        "  print('Start Training')\n",
        "  total_loss = 0\n",
        "  for i, (context, target) in enumerate(train_dataloader):\n",
        "    # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
        "    # new instance, you need to zero out the gradients from the old\n",
        "    # instance\n",
        "    model.zero_grad()\n",
        "\n",
        "    # Step 3. Run the forward pass, getting log probabilities over next\n",
        "    # words\n",
        "    print(context.shape)\n",
        "    log_probs = model(context)\n",
        "\n",
        "    # Step 4. Compute your loss function. (Again, Torch wants the target\n",
        "    # word wrapped in a tensor)\n",
        "    loss = loss_function(log_probs, torch.tensor([word_index[target]], dtype=torch.long))\n",
        "\n",
        "    # Step 5. Do the backward pass and update the gradient\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Get perplexity\n",
        "    perplexity = torch.exp(loss)\n",
        "\n",
        "    print('|epoch {}| {}/{} batches | lr {} | ms/batch {}| loss {:.2f}| ppl {:.2f}'.format(epoch+1, (i+1)*batch_size, num_batch*batch_size, learning_rate, time_diff_ms, loss.item(), perplexity))\n",
        "  total_loss = 0\n",
        "  for i, (context, target) in enumerate(valid_loader):\n",
        "    valid_context_idxs = torch.tensor([word_index[w] for w in context], dtype=torch.long)\n",
        "    valid_log_probs = model(valid_context_idxs)\n",
        "    loss = loss_function(valid_log_probs, torch.tensor([word_index[target]], dtype=torch.long) )\n",
        "    total_loss += loss.item()\n",
        "  avg_loss = total_loss/num_batch\n",
        "  avg_ppl = torch.exp(avg_loss)\n",
        "  print('-------------------------------------------------------------------')\n",
        "  print('|end of epoch {}| time: {}s| valid loss {:.2f} | valid ppl {:.2f}'.format(epoch+1, epoch_time, valid_loss, valid_ppl))\n",
        "  print('-------------------------------------------------------------------')\n",
        "print(losses)  # The loss decreased every iteration over the training data!\n",
        "\n",
        "# To get the embedding of a particular word, e.g. \"beauty\"\n",
        "print(model.embeddings.weight[word_index[\"abalobi\"]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training log:\n",
            "Start Training\n",
            "torch.Size([200, 2])\n",
            "torch.Size([200, 2])\n",
            "torch.Size([1, 4000])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-1d7b42cb071b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mlog_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# Step 4. Compute your loss function. (Again, Torch wants the target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-69-e1530c5e749f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0membeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mlog_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1845\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x4000 and 20x128)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmOT3I-9xTtM"
      },
      "source": [
        "## Create MLP Model\n",
        "Multilayer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYbZUQuXxTJW"
      },
      "source": [
        "# source: https://www.oreilly.com/library/view/natural-language-processing/9781491978221/ch04.html\n",
        "# source: https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html\n",
        "# source: https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/01-basics/feedforward_neural_network/main.py#L37-L49\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MultilayerPerceptron(nn.Module):\n",
        "    def __init__(self, context_size, embedding_size, vocabulary_size):\n",
        "        super(MultilayerPerceptron, self).__init__()\n",
        "        self.fc1 = nn.Linear(context_size, embedding_size)\n",
        "        self.fc2 = nn.Linear(embedding_size, vocabulary_size)\n",
        "\n",
        "    def forward(self, x_in):\n",
        "        intermediate = F.relu(self.fc1(x_in))\n",
        "        output = self.fc2(intermediate)\n",
        "        return  F.softmax(output, dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lP76tQ69yDqi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75e08c5c-d438-474a-c86f-f42c0b31920a"
      },
      "source": [
        "batch_size = 3 # number of samples input at once\n",
        "input_dim = 6\n",
        "hidden_dim = 3\n",
        "output_dim = 50\n",
        "\n",
        "# Initialize model\n",
        "mlp = MultilayerPerceptron(input_dim, hidden_dim, output_dim)\n",
        "print(mlp)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MultilayerPerceptron(\n",
            "  (fc1): Linear(in_features=6, out_features=3, bias=True)\n",
            "  (fc2): Linear(in_features=3, out_features=50, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XJxiCnDydA5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b23535ee-5103-4f0e-de3e-9afae39dca81"
      },
      "source": [
        "def describe(x):\n",
        "    print(\"Type: {}\".format(x.type()))\n",
        "    print(\"Shape/size: {}\".format(x.shape))\n",
        "    print(\"Values: \\n{}\".format(x))\n",
        "\n",
        "###########################\n",
        "#Format                   #\n",
        "#Batch1: \n",
        "#[ [word1, word 2, tagert],\n",
        "#  [word1, word 2, tagert],\n",
        "#  [word1, word 2, tagert],\n",
        "#]\n",
        "\n",
        "x_input = torch.rand(batch_size, input_dim)\n",
        "describe(x_input)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Type: torch.FloatTensor\n",
            "Shape/size: torch.Size([3, 6])\n",
            "Values: \n",
            "tensor([[0.8654, 0.9656, 0.4177, 0.3434, 0.9115, 0.7048],\n",
            "        [0.2483, 0.4000, 0.4714, 0.8006, 0.0089, 0.3843],\n",
            "        [0.2623, 0.7188, 0.9339, 0.5699, 0.2633, 0.6035]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCzqQ1mXyfDV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96709b1e-be09-4a68-8ce0-83566ea87f9d"
      },
      "source": [
        "y_output = mlp(x_input)\n",
        "describe(y_output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Type: torch.FloatTensor\n",
            "Shape/size: torch.Size([3, 50])\n",
            "Values: \n",
            "tensor([[0.0239, 0.0413, 0.0146, 0.0167, 0.0166, 0.0132, 0.0379, 0.0147, 0.0109,\n",
            "         0.0185, 0.0095, 0.0062, 0.0070, 0.0320, 0.0161, 0.0112, 0.0138, 0.0076,\n",
            "         0.0473, 0.0206, 0.0067, 0.0398, 0.0180, 0.0255, 0.0126, 0.0543, 0.0143,\n",
            "         0.0131, 0.0216, 0.0137, 0.0202, 0.0120, 0.0078, 0.0069, 0.0112, 0.0141,\n",
            "         0.0108, 0.0115, 0.0291, 0.0367, 0.0293, 0.0426, 0.0154, 0.0271, 0.0158,\n",
            "         0.0189, 0.0142, 0.0425, 0.0091, 0.0256],\n",
            "        [0.0226, 0.0428, 0.0120, 0.0145, 0.0161, 0.0119, 0.0340, 0.0142, 0.0101,\n",
            "         0.0168, 0.0089, 0.0062, 0.0063, 0.0338, 0.0169, 0.0110, 0.0125, 0.0084,\n",
            "         0.0496, 0.0237, 0.0066, 0.0409, 0.0202, 0.0206, 0.0135, 0.0618, 0.0149,\n",
            "         0.0141, 0.0217, 0.0140, 0.0169, 0.0115, 0.0089, 0.0073, 0.0098, 0.0144,\n",
            "         0.0095, 0.0096, 0.0327, 0.0361, 0.0321, 0.0468, 0.0134, 0.0251, 0.0162,\n",
            "         0.0191, 0.0167, 0.0414, 0.0084, 0.0229],\n",
            "        [0.0221, 0.0435, 0.0111, 0.0137, 0.0157, 0.0113, 0.0329, 0.0140, 0.0097,\n",
            "         0.0163, 0.0087, 0.0061, 0.0060, 0.0347, 0.0173, 0.0109, 0.0121, 0.0086,\n",
            "         0.0506, 0.0247, 0.0065, 0.0409, 0.0209, 0.0191, 0.0138, 0.0642, 0.0151,\n",
            "         0.0145, 0.0218, 0.0139, 0.0158, 0.0113, 0.0093, 0.0074, 0.0093, 0.0143,\n",
            "         0.0090, 0.0090, 0.0343, 0.0362, 0.0334, 0.0487, 0.0129, 0.0245, 0.0164,\n",
            "         0.0191, 0.0177, 0.0408, 0.0081, 0.0221]], grad_fn=<SoftmaxBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "It4cDh0X8pFi"
      },
      "source": [
        "Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKBrTI8S8wBI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "outputId": "95494046-7852-4c3d-a9b2-a9702752ccd4"
      },
      "source": [
        "import datetime\n",
        "\n",
        "learning_rate = 20\n",
        "\n",
        "# Use cross entroy loss as loss function\n",
        "cross_entropy_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "# Use stochastic gradient descent as optimization function\n",
        "gradient_decent_optimiser = torch.optim.SGD(mlp.parameters(), lr=learning_rate)\n",
        "\n",
        "print('Training log:')\n",
        "num_batches = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "  print('Start Training')\n",
        "  epoch_start_time = datetime.datetime.now()\n",
        "\n",
        "  for i, (example_input, example_output) in enumerate(train_loader):\n",
        "    start_time = datetime.datetime.now()\n",
        "\n",
        "    # Forward pass\n",
        "    output = mlp(example_input)\n",
        "    loss = cross_entropy_loss(output, example_output)\n",
        "\n",
        "    # Backward Propagation and optimisation\n",
        "    gradient_decent_optimiser.zero_grad\n",
        "    loss.backward()\n",
        "    gradient_decent_optimiser.step()\n",
        "    \n",
        "    # Calculate time\n",
        "    end_time = datetime.datetim.now()\n",
        "    time_diff = (end_time - start_time)\n",
        "    time_diff_ms = time_diff * 1000\n",
        "\n",
        "    perplexity = torch.exp(loss)\n",
        "    if i == (num_batches - 1):\n",
        "      print('|epoch {}| {}/{} batches | lr {} | ms/batch {}| loss {:.2f}| ppl {:.2f}'.format(epoch+1, (i+1)*batch_size, num_batch, learning_rate, time_diff_ms, loss.item(), perplexity))\n",
        "  \n",
        "  for i, (valid_input, valid_output) in enumerate(valid_loader):\n",
        "    output = mlp(valid_input)\n",
        "    valid_loss = cross_entropy_loss(output, valid_output)\n",
        "    \n",
        "\n",
        "  epoch_end_time = datatime.datetime.now()\n",
        "  epoch_time = (epoch_end_time - epoch_start_time)\n",
        "  print('-------------------------------------------------------------------')\n",
        "  print('|end of epoch {}| time: {}s| valid loss {:.2f} | valid ppl {:.2f}'.format(epoch+1, epoch_time, valid_loss, valid_ppl))\n",
        "  print('-------------------------------------------------------------------')\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training log:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-612d8f829b39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training log:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mnum_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Start Training'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
          ]
        }
      ]
    }
  ]
}
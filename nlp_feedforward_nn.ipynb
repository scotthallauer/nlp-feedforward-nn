{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp-feedforward-nn.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "aYZ_fg0Cg8GQ",
        "gmOT3I-9xTtM"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scotthallauer/nlp-feedforward-nn/blob/main/nlp_feedforward_nn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQJwOwG5ApUg"
      },
      "source": [
        "# NLP Assignment 2\n",
        "\n",
        "**Authors:** Scott Hallauer (HLLSCO001) and Steve Wang (WNGSHU003)\n",
        "\n",
        "**Date:** 21 June 2021"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TZir7sSW_7z"
      },
      "source": [
        "## Set-Up\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YACofn5oRF8W"
      },
      "source": [
        "Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gr-77pCjAnKz"
      },
      "source": [
        "import urllib\n",
        "import re\n",
        "import datetime\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zp-QRNj2RJaH"
      },
      "source": [
        "Set model parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXppOZ-zRL1Z"
      },
      "source": [
        "# Manual parameters\n",
        "NGRAM_SIZE = 3\n",
        "EMBEDDING_DIM = 10\n",
        "BATCH_SIZE = 400\n",
        "LEARNING_RATE = 0.1\n",
        "\n",
        "# Calculated parameters\n",
        "CONTEXT_SIZE = NGRAM_SIZE-1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiPZG9y6g_sW"
      },
      "source": [
        "train_url = \"https://raw.githubusercontent.com/scotthallauer/nlp-feedforward-nn/main/xh-data/nchlt_text.xh.train\"\n",
        "valid_url = \"https://raw.githubusercontent.com/scotthallauer/nlp-feedforward-nn/main/xh-data/nchlt_text.xh.valid\"\n",
        "test_url = \"https://raw.githubusercontent.com/scotthallauer/nlp-feedforward-nn/main/xh-data/nchlt_text.xh.test\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYZ_fg0Cg8GQ"
      },
      "source": [
        "## Prepare Datasets (OLD)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2woZxSklaDL"
      },
      "source": [
        "def load_dataset(url):\n",
        "  \"\"\"Load a raw dataset from the given URL and convert it into a list of lower-case, punctuation-cleaned sentences.\"\"\"\n",
        "  dataset = []\n",
        "  with urllib.request.urlopen(url) as data:\n",
        "    for line in data:\n",
        "      input_sentence = line.decode().split()\n",
        "      output_sentence = []\n",
        "      for input_word in input_sentence:\n",
        "        output_word = input_word.lower()\n",
        "        output_word = re.sub(r\"(\\.|\\,|\\:|\\?|\\)|\\\"|\\-|\\!|\\/|\\')+$\", \"\", output_word)\n",
        "        output_word = re.sub(r\"^(\\(|\\\"|\\-|\\/|\\')+\", \"\", output_word)\n",
        "        if len(output_word) > 0:\n",
        "          output_sentence.append(output_word)\n",
        "      dataset.append(output_sentence)\n",
        "  return dataset\n",
        "\n",
        "def prepare_dataset(dataset):\n",
        "  \"\"\"Replace all words that only occur once in the dataset with the <UNK> token.\"\"\"\n",
        "  word_counts = {}\n",
        "  for sentence in dataset:\n",
        "    for word in sentence:\n",
        "      if word not in word_counts:\n",
        "        word_counts[word] = 1\n",
        "      else:\n",
        "        word_counts[word] += 1\n",
        "  for i in range(len(dataset)):\n",
        "    for j in range(len(dataset[i])):\n",
        "      if word_counts[dataset[i][j]] == 1:\n",
        "        dataset[i][j] = \"<UNK>\"\n",
        "  return dataset\n",
        "\n",
        "def get_word_index(dataset):\n",
        "  word_index = {}\n",
        "  index = 0\n",
        "  for sentence in dataset:\n",
        "    for word in sentence:\n",
        "      if word not in word_index:\n",
        "        word_index[word] = index\n",
        "        index += 1\n",
        "  return word_index\n",
        "\n",
        "def get_word_list(word_index):\n",
        "  word_list = []\n",
        "  for i in range(len(word_index)):\n",
        "    word_list.append(\"\");\n",
        "  for word in word_index:\n",
        "    word_list[word_index[word]] = word\n",
        "  return word_list\n",
        "\n",
        "def get_n_grams(dataset, n):\n",
        "  n_grams = []\n",
        "  for sentence in dataset:\n",
        "    if len(sentence) >= n:\n",
        "      for i in range(len(sentence)-(n-1)):\n",
        "        n_grams.append((sentence[i:i+(n-1)], sentence[i+(n-1)]))\n",
        "  return n_grams\n",
        "\n",
        "def get_n_gram_counts(n_grams):\n",
        "  n_gram_counts = {}\n",
        "  for context, target in n_grams:\n",
        "    n_gram_key = repr((context, target))\n",
        "    if n_gram_key not in n_gram_counts:\n",
        "      n_gram_counts[n_gram_key] = 1\n",
        "    else:\n",
        "      n_gram_counts[n_gram_key] += 1\n",
        "  return n_gram_counts\n",
        "\n",
        "def get_n_gram_context_counts(n_grams):\n",
        "  n_gram_context_counts = {}\n",
        "  for context, target in n_grams:\n",
        "    n_gram_context_key = repr(context)\n",
        "    if n_gram_context_key not in n_gram_context_counts:\n",
        "      n_gram_context_counts[n_gram_context_key] = 1\n",
        "    else:\n",
        "      n_gram_context_counts[n_gram_context_key] += 1\n",
        "  return n_gram_context_counts\n",
        "\n",
        "def get_n_gram_count(n_gram, n_gram_counts):\n",
        "  n_gram_key = repr(n_gram)\n",
        "  if n_gram_key not in n_gram_counts:\n",
        "    return 0\n",
        "  else:\n",
        "    return n_gram_counts[n_gram_key]\n",
        "\n",
        "def get_n_gram_probability(context, target, n_gram_counts, n_gram_context_counts):\n",
        "  \"\"\"P(w_n | w_1 w_2 ... w_n-1)\"\"\"\n",
        "  n_gram = (context, target)\n",
        "  n_gram_count = get_n_gram_count(n_gram, n_gram_counts)\n",
        "  n_gram_context_count = get_n_gram_count(context, n_gram_context_counts)\n",
        "  if n_gram_count == 0 or n_gram_context_count == 0:\n",
        "    return 0\n",
        "  else:\n",
        "    return n_gram_count / n_gram_context_count\n",
        "\n",
        "#def get_n_gram_dist(dataset, n):\n",
        "#  \"\"\"P(w_n | w_1 w_2 ... w_n-1)\"\"\"\n",
        "#  word_index = get_word_index(dataset)\n",
        "#  word_list = get_word_list(word_index)\n",
        "#  n_gram_list = get_n_gram_list(dataset, n)\n",
        "#  n_gram_counts = get_n_gram_counts(n_gram_list)\n",
        "#  n_sub1_gram_list = get_n_gram_list(dataset, n-1)\n",
        "#  n_sub1_gram_counts = get_n_gram_counts(n_gram_list)\n",
        "#  n_gram_dist = {}\n",
        "#  for pre_sequence in n_sub1_gram_list:\n",
        "#    pre_sequence_key = repr(pre_sequence)\n",
        "#    if pre_sequence_key not in n_gram_dist:\n",
        "#      n_gram_dist[pre_sequence_key] = {}\n",
        "#      for word in word_list:\n",
        "#        n_gram = pre_sequence + [word]\n",
        "#        pre_sequence_count = get_n_gram_count(pre_sequence, n_sub1_gram_counts)\n",
        "#        n_gram_count = get_n_gram_count(n_gram, n_gram_counts)\n",
        "#        if pre_sequence_count == 0:\n",
        "#          n_gram_dist[pre_sequence_key][word] = 0\n",
        "#        else:\n",
        "#          n_gram_dist[pre_sequence_key][word] = n_gram_count / pre_sequence_count\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9FV2j8GP-o6"
      },
      "source": [
        "Load and clean the training dataset. Then replace all words which only occur once with the \\<UNK\\> token."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVlMJBuljofa"
      },
      "source": [
        "train_data = load_dataset(train_url)\n",
        "train_data = prepare_dataset(train_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3pfofCAQll8"
      },
      "source": [
        "From the training dataset, we now extract the vocabulary and generate a dictionary with the word-to-index mappings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nc5yN0NoJbPe"
      },
      "source": [
        "word_index = get_word_index(train_data)\n",
        "vocabulary = get_word_list(word_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-fvb7IaQ2XQ"
      },
      "source": [
        "Finally, we extract all occurrences of n-grams from the training dataset and get the corresponding counts of both the full n-grams and the prefix (from 1 to n-1) n-grams."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_J9kg_Wevrl"
      },
      "source": [
        "n_grams = get_n_grams(train_data, N_GRAM_SIZE)\n",
        "n_gram_counts = get_n_gram_counts(n_grams)\n",
        "context_counts = get_n_gram_context_counts(n_grams)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sny3pWzsQYyB"
      },
      "source": [
        "All code blocks below are just for exploring the newly created representations of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpFojVGlMkmq",
        "outputId": "ecbb7ac2-8332-45f7-92d7-fd1dabca0dba"
      },
      "source": [
        "print(len(vocabulary))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "62244\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ue6U5zfOQ8KX",
        "outputId": "970df7f9-96b7-4506-88ec-aeda50cef8ef"
      },
      "source": [
        "print(train_data[70835])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['xa', '<UNK>', 'abalobi', 'ababandakanyekayo', 'neegiyeri', 'kufuneka', '<UNK>', 'ezomeleleyo', 'ezibukhali', 'kunye/okanye', 'amazembe', 'ngexesha', 'lokusebenza', 'ukusika', 'iihaki', 'ezithintelayo', 'okanye', 'imitya', 'yokuloba']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsyyNlkzi3mD",
        "outputId": "ac5e4069-1ea4-4493-b083-af52edf77c8f"
      },
      "source": [
        "print(n_grams[5])\n",
        "print(n_gram_counts[repr(n_grams[5])])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(['ngocoselelo', 'njengoko'], 'siqulethe')\n",
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05Md_jrLwb6a",
        "outputId": "47e4e482-90ff-483e-a700-47b0036ec46f"
      },
      "source": [
        "context = [\"xa\", \"<UNK>\"]\n",
        "target = \"abalobi\"\n",
        "print(get_n_gram_probability(context, target, n_gram_counts, context_counts))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0024875621890547263\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "priIoyy_EKrO"
      },
      "source": [
        "## Prepare Datasets (NEW)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K63_zdwFEOko"
      },
      "source": [
        "def load_dataset(url):\n",
        "  dataset = []\n",
        "  with urllib.request.urlopen(url) as data:\n",
        "    for line in data:\n",
        "      input_sentence = line.decode().split()\n",
        "      output_sentence = []\n",
        "      for input_word in input_sentence:\n",
        "        output_word = input_word.lower()\n",
        "        output_word = re.sub(r\"(\\.|\\,|\\:|\\?|\\)|\\\"|\\-|\\!|\\/|\\')+$\", \"\", output_word)\n",
        "        output_word = re.sub(r\"^(\\(|\\\"|\\-|\\/|\\')+\", \"\", output_word)\n",
        "        if len(output_word) > 0:\n",
        "          output_sentence.append(output_word)\n",
        "      dataset.append(output_sentence)\n",
        "  word_counts = {}\n",
        "  for sentence in dataset:\n",
        "    for word in sentence:\n",
        "      if word not in word_counts:\n",
        "        word_counts[word] = 1\n",
        "      else:\n",
        "        word_counts[word] += 1\n",
        "  for i in range(len(dataset)):\n",
        "    for j in range(len(dataset[i])):\n",
        "      if word_counts[dataset[i][j]] == 1:\n",
        "        dataset[i][j] = \"<unk>\"\n",
        "  return dataset\n",
        "\n",
        "def get_word_index(sentences):\n",
        "  word_index = {}\n",
        "  index = 0\n",
        "  for sentence in sentences:\n",
        "    for word in sentence:\n",
        "      if word not in word_index:\n",
        "        word_index[word] = index\n",
        "        index += 1\n",
        "  return word_index\n",
        "\n",
        "def get_vocab(word_index):\n",
        "  vocab = []\n",
        "  for i in range(len(word_index)):\n",
        "    vocab.append(\"\");\n",
        "  for word in word_index:\n",
        "    vocab[word_index[word]] = word\n",
        "  return vocab\n",
        "\n",
        "def get_ngrams(sentences, n):\n",
        "  ngrams = []\n",
        "  for sentence in sentences:\n",
        "    if len(sentence) >= n:\n",
        "      for i in range(len(sentence)-(n-1)):\n",
        "        ngrams.append((sentence[i:i+(n-1)], sentence[i+(n-1)]))\n",
        "  return ngrams"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HTdpgTHEVmI"
      },
      "source": [
        "class XhosaTextDataset(Dataset):\n",
        "  def __init__(self, dataset_url, ngram_size):\n",
        "    self.sentences = load_dataset(dataset_url)\n",
        "    self.word_index = get_word_index(self.sentences)\n",
        "    self.vocab = get_vocab(self.word_index)\n",
        "    self.ngrams = get_ngrams(self.sentences, ngram_size)\n",
        "    self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.ngrams)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    context = []\n",
        "    for word in self.ngrams[idx][0]:\n",
        "      context.append(self.word_index[word])\n",
        "    target = self.word_index[self.ngrams[idx][1]]\n",
        "    return torch.tensor(context, dtype=torch.long, device=torch.device(self.device)), torch.tensor(target, dtype=torch.long, device=torch.device(self.device))\n",
        "\n",
        "  def vocab_size(self):\n",
        "    return len(self.vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eflxhEc8EcwB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ee1e7f4-3e10-4bb3-dfe0-ab6e7a44328f"
      },
      "source": [
        "train_dataset = XhosaTextDataset(\n",
        "  dataset_url=train_url, \n",
        "  ngram_size=NGRAM_SIZE,\n",
        ")\n",
        "print(f\"Training Dataset: {train_dataset.__len__()} ngrams.\")\n",
        "\n",
        "valid_dataset = XhosaTextDataset(\n",
        "  dataset_url=valid_url, \n",
        "  ngram_size=NGRAM_SIZE,\n",
        ")\n",
        "print(f\"Validation Dataset: {valid_dataset.__len__()} ngrams.\")\n",
        "\n",
        "test_dataset = XhosaTextDataset(\n",
        "  dataset_url=test_url, \n",
        "  ngram_size=NGRAM_SIZE,\n",
        ")\n",
        "print(f\"Testing Dataset: {test_dataset.__len__()} ngrams.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Dataset: 859472 ngrams.\n",
            "Validation Dataset: 47430 ngrams.\n",
            "Testing Dataset: 47911 ngrams.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ThOPpfDHH3F",
        "outputId": "dd7c3abc-f83f-430f-c0f5-dbf2c577b3dd"
      },
      "source": [
        "train_dataset.__getitem__(400)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([22, 46], device='cuda:0'), tensor(319, device='cuda:0'))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJ7LgtjL2BNT"
      },
      "source": [
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6_3PMOPFYw4",
        "outputId": "a2316e6d-b170-4485-e163-8efa70e52db6"
      },
      "source": [
        "train_contexts, train_targets = next(iter(train_dataloader))\n",
        "print(f\"Contexts batch shape: {train_contexts.size()}\")\n",
        "print(f\"Targets batch shape: {train_targets.size()}\")\n",
        "context = train_contexts[0]\n",
        "target = train_targets[0]\n",
        "print(f\"Context: {context}\")\n",
        "print(f\"Target: {target}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Contexts batch shape: torch.Size([200, 2])\n",
            "Targets batch shape: torch.Size([200])\n",
            "Context: tensor([10930, 36206], device='cuda:0')\n",
            "Target: 4172\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yr98KqLGO4Ny"
      },
      "source": [
        "## Define Neural Network Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTr9iz9_jY7r"
      },
      "source": [
        "# source: https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html\n",
        "class NGramLanguageModeler(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, context_size, batch_size):\n",
        "    super(NGramLanguageModeler, self).__init__()\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.context_size = context_size\n",
        "    self.batch_size = batch_size\n",
        "    self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
        "    self.linear2 = nn.Linear(128, vocab_size)\n",
        "  \n",
        "  def forward(self, inputs):\n",
        "    embeds = self.embeddings(inputs).view((len(inputs), self.context_size * self.embedding_dim))\n",
        "    out = F.relu(self.linear1(embeds))\n",
        "    out = self.linear2(out)\n",
        "    log_probs = F.log_softmax(out, dim=1)\n",
        "    return log_probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3846zpWjc9H"
      },
      "source": [
        "## Train Neural Network Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9WRC3KHO9Zn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4abc015f-7aa1-426b-dfa0-7c46107efb20"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "batch_log_interval = 200\n",
        "\n",
        "model = NGramLanguageModeler(train_dataset.vocab_size(), EMBEDDING_DIM, CONTEXT_SIZE, BATCH_SIZE).to(device)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "for epoch_i in range(10):\n",
        "\n",
        "  print('Start Training')\n",
        "\n",
        "  # Start timing epoch\n",
        "  epoch_start_time = datetime.datetime.now()\n",
        "\n",
        "  # TRAINING PHASE\n",
        "  model.train()\n",
        "  total_batch_elapsed_time_ms = 0\n",
        "  total_batch_loss = 0\n",
        "  total_batch_ppl = 0\n",
        "\n",
        "  for batch_i, (train_context, train_target) in enumerate(train_dataloader):\n",
        "\n",
        "    # Start timing batch\n",
        "    batch_start_time = datetime.datetime.now()\n",
        "\n",
        "    # Zero gradients from old input\n",
        "    model.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    log_probs = model(train_context)\n",
        "\n",
        "    # Get loss\n",
        "    loss = loss_function(log_probs, train_target)\n",
        "    total_batch_loss += loss.item()\n",
        "\n",
        "    # Get perplexity\n",
        "    ppl = torch.exp(loss)\n",
        "    total_batch_ppl += ppl.item()\n",
        "\n",
        "    # Backward propagation\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    # Stop timing batch\n",
        "    batch_end_time = datetime.datetime.now()\n",
        "    batch_elapsed_time = (batch_end_time - batch_start_time)\n",
        "    batch_elapsed_time_ms = batch_elapsed_time.total_seconds() * 1000\n",
        "    total_batch_elapsed_time_ms += batch_elapsed_time_ms\n",
        "\n",
        "    # Log training stats in batch intervals\n",
        "    if ((batch_i+1) % batch_log_interval == 0):\n",
        "      print(\"| epoch {} | {}/ {} batches | lr {:.2f} | ms/batch {:.2f} | loss {:.2f} | ppl {:.2f}\".format(epoch_i+1, batch_i+1, len(train_dataloader), LEARNING_RATE, total_batch_elapsed_time_ms/batch_log_interval, total_batch_loss/batch_log_interval, total_batch_ppl/batch_log_interval))\n",
        "      total_batch_elapsed_time_ms = 0\n",
        "      total_batch_loss = 0\n",
        "      total_batch_ppl = 0\n",
        "  \n",
        "  # VALIDATION PHASE\n",
        "  model.eval()\n",
        "  total_valid_loss = 0\n",
        "  total_valid_ppl = 0\n",
        "\n",
        "  for batch_i, (valid_context, valid_target) in enumerate(valid_dataloader):\n",
        "\n",
        "    # Forward pass\n",
        "    log_probs = model(valid_context)\n",
        "\n",
        "    # Get loss\n",
        "    loss = loss_function(log_probs, valid_target)\n",
        "    total_valid_loss += loss.item()\n",
        "\n",
        "    # Get perplexity\n",
        "    ppl = torch.exp(loss)\n",
        "    total_valid_ppl += ppl.item()\n",
        "\n",
        "  # Stop timing epoch\n",
        "  epoch_end_time = datetime.datetime.now()\n",
        "  epoch_elapsed_time = (epoch_end_time - epoch_start_time)\n",
        "  epoch_elapsed_time_s = epoch_elapsed_time.total_seconds()\n",
        "\n",
        "  # Log validation stats at end of each epoch\n",
        "  print(\"-------------------------------------------------------------------\")\n",
        "  print(\"| end of epoch {} | time: {:.2f}s | valid loss {:.2f} | valid ppl {:.2f}\".format(epoch_i+1, epoch_elapsed_time_s, total_valid_loss/len(valid_dataloader), total_valid_ppl/len(valid_dataloader)))\n",
        "  print(\"-------------------------------------------------------------------\")\n",
        "\n",
        "# TESTING PHASE\n",
        "model.eval()\n",
        "total_test_loss = 0\n",
        "total_test_ppl = 0\n",
        "\n",
        "for batch_i, (test_context, test_target) in enumerate(test_dataloader):\n",
        "\n",
        "  # Forward pass\n",
        "  log_probs = model(test_context)\n",
        "\n",
        "  # Get loss\n",
        "  loss = loss_function(log_probs, test_target)\n",
        "  total_test_loss += loss.item()\n",
        "\n",
        "  # Get perplexity\n",
        "  ppl = torch.exp(loss)\n",
        "  total_test_ppl += ppl.item()\n",
        "\n",
        "# Log testing stats at end of training\n",
        "print(\"=========================================================================================\")\n",
        "print(\"| End of training | test loss {:.2f} | test ppl {:.2f}\".format(total_test_loss/len(test_dataloader), total_test_ppl/len(test_dataloader)))\n",
        "print(\"=========================================================================================\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start Training\n",
            "| epoch 1 | 200/ 2149 batches | lr 0.10 | ms/batch 6.43 | loss 10.63 | ppl 42436.93\n",
            "| epoch 1 | 400/ 2149 batches | lr 0.10 | ms/batch 6.30 | loss 10.27 | ppl 28954.47\n",
            "| epoch 1 | 600/ 2149 batches | lr 0.10 | ms/batch 6.39 | loss 10.10 | ppl 24610.42\n",
            "| epoch 1 | 800/ 2149 batches | lr 0.10 | ms/batch 6.37 | loss 9.95 | ppl 21173.18\n",
            "| epoch 1 | 1000/ 2149 batches | lr 0.10 | ms/batch 6.40 | loss 9.80 | ppl 18134.81\n",
            "| epoch 1 | 1200/ 2149 batches | lr 0.10 | ms/batch 6.32 | loss 9.67 | ppl 15934.10\n",
            "| epoch 1 | 1400/ 2149 batches | lr 0.10 | ms/batch 6.36 | loss 9.53 | ppl 13872.95\n",
            "| epoch 1 | 1600/ 2149 batches | lr 0.10 | ms/batch 6.36 | loss 9.47 | ppl 13102.55\n",
            "| epoch 1 | 1800/ 2149 batches | lr 0.10 | ms/batch 6.32 | loss 9.38 | ppl 11992.14\n",
            "| epoch 1 | 2000/ 2149 batches | lr 0.10 | ms/batch 6.35 | loss 9.31 | ppl 11219.87\n",
            "-------------------------------------------------------------------\n",
            "| end of epoch 1 | time: 65.35s | valid loss 9.85 | valid ppl 19016.59\n",
            "-------------------------------------------------------------------\n",
            "Start Training\n",
            "| epoch 2 | 200/ 2149 batches | lr 0.10 | ms/batch 6.36 | loss 9.20 | ppl 9980.86\n",
            "| epoch 2 | 400/ 2149 batches | lr 0.10 | ms/batch 6.31 | loss 9.17 | ppl 9715.70\n",
            "| epoch 2 | 600/ 2149 batches | lr 0.10 | ms/batch 6.37 | loss 9.13 | ppl 9319.64\n",
            "| epoch 2 | 800/ 2149 batches | lr 0.10 | ms/batch 6.36 | loss 9.08 | ppl 8872.98\n",
            "| epoch 2 | 1000/ 2149 batches | lr 0.10 | ms/batch 6.35 | loss 9.07 | ppl 8760.89\n",
            "| epoch 2 | 1200/ 2149 batches | lr 0.10 | ms/batch 6.36 | loss 9.04 | ppl 8520.04\n",
            "| epoch 2 | 1400/ 2149 batches | lr 0.10 | ms/batch 6.32 | loss 9.02 | ppl 8368.33\n",
            "| epoch 2 | 1600/ 2149 batches | lr 0.10 | ms/batch 6.41 | loss 8.99 | ppl 8068.02\n",
            "| epoch 2 | 1800/ 2149 batches | lr 0.10 | ms/batch 6.33 | loss 8.97 | ppl 7989.69\n",
            "| epoch 2 | 2000/ 2149 batches | lr 0.10 | ms/batch 6.33 | loss 8.94 | ppl 7718.05\n",
            "-------------------------------------------------------------------\n",
            "| end of epoch 2 | time: 65.44s | valid loss 9.80 | valid ppl 18115.85\n",
            "-------------------------------------------------------------------\n",
            "Start Training\n",
            "| epoch 3 | 200/ 2149 batches | lr 0.10 | ms/batch 6.34 | loss 8.90 | ppl 7419.06\n",
            "| epoch 3 | 400/ 2149 batches | lr 0.10 | ms/batch 6.31 | loss 8.89 | ppl 7361.86\n",
            "| epoch 3 | 600/ 2149 batches | lr 0.10 | ms/batch 6.37 | loss 8.91 | ppl 7459.46\n",
            "| epoch 3 | 800/ 2149 batches | lr 0.10 | ms/batch 6.34 | loss 8.89 | ppl 7298.58\n",
            "| epoch 3 | 1000/ 2149 batches | lr 0.10 | ms/batch 6.33 | loss 8.90 | ppl 7401.04\n",
            "| epoch 3 | 1200/ 2149 batches | lr 0.10 | ms/batch 6.35 | loss 8.87 | ppl 7199.89\n",
            "| epoch 3 | 1400/ 2149 batches | lr 0.10 | ms/batch 6.34 | loss 8.87 | ppl 7202.46\n",
            "| epoch 3 | 1600/ 2149 batches | lr 0.10 | ms/batch 6.33 | loss 8.86 | ppl 7131.20\n",
            "| epoch 3 | 1800/ 2149 batches | lr 0.10 | ms/batch 6.34 | loss 8.85 | ppl 7086.62\n",
            "| epoch 3 | 2000/ 2149 batches | lr 0.10 | ms/batch 6.33 | loss 8.86 | ppl 7110.26\n",
            "-------------------------------------------------------------------\n",
            "| end of epoch 3 | time: 65.22s | valid loss 9.84 | valid ppl 18902.43\n",
            "-------------------------------------------------------------------\n",
            "Start Training\n",
            "| epoch 4 | 200/ 2149 batches | lr 0.10 | ms/batch 6.36 | loss 8.81 | ppl 6782.62\n",
            "| epoch 4 | 400/ 2149 batches | lr 0.10 | ms/batch 6.36 | loss 8.81 | ppl 6798.36\n",
            "| epoch 4 | 600/ 2149 batches | lr 0.10 | ms/batch 6.34 | loss 8.80 | ppl 6702.61\n",
            "| epoch 4 | 800/ 2149 batches | lr 0.10 | ms/batch 6.34 | loss 8.80 | ppl 6696.72\n",
            "| epoch 4 | 1000/ 2149 batches | lr 0.10 | ms/batch 6.31 | loss 8.79 | ppl 6640.32\n",
            "| epoch 4 | 1200/ 2149 batches | lr 0.10 | ms/batch 6.34 | loss 8.78 | ppl 6579.16\n",
            "| epoch 4 | 1400/ 2149 batches | lr 0.10 | ms/batch 6.33 | loss 8.79 | ppl 6625.82\n",
            "| epoch 4 | 1600/ 2149 batches | lr 0.10 | ms/batch 6.35 | loss 8.78 | ppl 6583.98\n",
            "| epoch 4 | 1800/ 2149 batches | lr 0.10 | ms/batch 6.35 | loss 8.78 | ppl 6580.46\n",
            "| epoch 4 | 2000/ 2149 batches | lr 0.10 | ms/batch 6.35 | loss 8.78 | ppl 6589.47\n",
            "-------------------------------------------------------------------\n",
            "| end of epoch 4 | time: 65.18s | valid loss 9.79 | valid ppl 17961.55\n",
            "-------------------------------------------------------------------\n",
            "Start Training\n",
            "| epoch 5 | 200/ 2149 batches | lr 0.10 | ms/batch 6.35 | loss 8.76 | ppl 6463.04\n",
            "| epoch 5 | 400/ 2149 batches | lr 0.10 | ms/batch 6.35 | loss 8.75 | ppl 6394.07\n",
            "| epoch 5 | 600/ 2149 batches | lr 0.10 | ms/batch 6.33 | loss 8.74 | ppl 6342.27\n",
            "| epoch 5 | 800/ 2149 batches | lr 0.10 | ms/batch 6.39 | loss 8.72 | ppl 6205.69\n",
            "| epoch 5 | 1000/ 2149 batches | lr 0.10 | ms/batch 6.35 | loss 8.75 | ppl 6348.69\n",
            "| epoch 5 | 1200/ 2149 batches | lr 0.10 | ms/batch 6.34 | loss 8.71 | ppl 6148.85\n",
            "| epoch 5 | 1400/ 2149 batches | lr 0.10 | ms/batch 6.34 | loss 8.72 | ppl 6159.12\n",
            "| epoch 5 | 1600/ 2149 batches | lr 0.10 | ms/batch 6.31 | loss 8.69 | ppl 6031.35\n",
            "| epoch 5 | 1800/ 2149 batches | lr 0.10 | ms/batch 6.33 | loss 8.73 | ppl 6219.29\n",
            "| epoch 5 | 2000/ 2149 batches | lr 0.10 | ms/batch 6.36 | loss 8.70 | ppl 6064.78\n",
            "-------------------------------------------------------------------\n",
            "| end of epoch 5 | time: 65.40s | valid loss 9.84 | valid ppl 18953.77\n",
            "-------------------------------------------------------------------\n",
            "Start Training\n",
            "| epoch 6 | 200/ 2149 batches | lr 0.10 | ms/batch 6.32 | loss 8.68 | ppl 5966.18\n",
            "| epoch 6 | 400/ 2149 batches | lr 0.10 | ms/batch 6.35 | loss 8.68 | ppl 5960.53\n",
            "| epoch 6 | 600/ 2149 batches | lr 0.10 | ms/batch 6.38 | loss 8.69 | ppl 5999.30\n",
            "| epoch 6 | 800/ 2149 batches | lr 0.10 | ms/batch 6.35 | loss 8.67 | ppl 5898.83\n",
            "| epoch 6 | 1000/ 2149 batches | lr 0.10 | ms/batch 6.34 | loss 8.65 | ppl 5773.22\n",
            "| epoch 6 | 1200/ 2149 batches | lr 0.10 | ms/batch 6.35 | loss 8.67 | ppl 5897.56\n",
            "| epoch 6 | 1400/ 2149 batches | lr 0.10 | ms/batch 6.33 | loss 8.67 | ppl 5907.80\n",
            "| epoch 6 | 1600/ 2149 batches | lr 0.10 | ms/batch 6.34 | loss 8.67 | ppl 5855.11\n",
            "| epoch 6 | 1800/ 2149 batches | lr 0.10 | ms/batch 6.39 | loss 8.66 | ppl 5846.47\n",
            "| epoch 6 | 2000/ 2149 batches | lr 0.10 | ms/batch 6.34 | loss 8.67 | ppl 5887.51\n",
            "-------------------------------------------------------------------\n",
            "| end of epoch 6 | time: 65.21s | valid loss 9.78 | valid ppl 17876.43\n",
            "-------------------------------------------------------------------\n",
            "Start Training\n",
            "| epoch 7 | 200/ 2149 batches | lr 0.10 | ms/batch 6.34 | loss 8.63 | ppl 5679.72\n",
            "| epoch 7 | 400/ 2149 batches | lr 0.10 | ms/batch 6.43 | loss 8.62 | ppl 5599.22\n",
            "| epoch 7 | 600/ 2149 batches | lr 0.10 | ms/batch 6.44 | loss 8.63 | ppl 5645.48\n",
            "| epoch 7 | 800/ 2149 batches | lr 0.10 | ms/batch 6.41 | loss 8.61 | ppl 5542.63\n",
            "| epoch 7 | 1000/ 2149 batches | lr 0.10 | ms/batch 6.43 | loss 8.62 | ppl 5628.05\n",
            "| epoch 7 | 1200/ 2149 batches | lr 0.10 | ms/batch 6.35 | loss 8.62 | ppl 5588.50\n",
            "| epoch 7 | 1400/ 2149 batches | lr 0.10 | ms/batch 6.35 | loss 8.60 | ppl 5499.34\n",
            "| epoch 7 | 1600/ 2149 batches | lr 0.10 | ms/batch 6.33 | loss 8.61 | ppl 5569.46\n",
            "| epoch 7 | 1800/ 2149 batches | lr 0.10 | ms/batch 6.39 | loss 8.62 | ppl 5627.74\n",
            "| epoch 7 | 2000/ 2149 batches | lr 0.10 | ms/batch 6.36 | loss 8.60 | ppl 5494.27\n",
            "-------------------------------------------------------------------\n",
            "| end of epoch 7 | time: 65.82s | valid loss 9.82 | valid ppl 18462.85\n",
            "-------------------------------------------------------------------\n",
            "Start Training\n",
            "| epoch 8 | 200/ 2149 batches | lr 0.10 | ms/batch 6.37 | loss 8.60 | ppl 5459.37\n",
            "| epoch 8 | 400/ 2149 batches | lr 0.10 | ms/batch 6.34 | loss 8.57 | ppl 5342.06\n",
            "| epoch 8 | 600/ 2149 batches | lr 0.10 | ms/batch 6.38 | loss 8.57 | ppl 5346.07\n",
            "| epoch 8 | 800/ 2149 batches | lr 0.10 | ms/batch 6.36 | loss 8.56 | ppl 5284.56\n",
            "| epoch 8 | 1000/ 2149 batches | lr 0.10 | ms/batch 6.34 | loss 8.56 | ppl 5295.31\n",
            "| epoch 8 | 1200/ 2149 batches | lr 0.10 | ms/batch 6.36 | loss 8.55 | ppl 5249.07\n",
            "| epoch 8 | 1400/ 2149 batches | lr 0.10 | ms/batch 6.33 | loss 8.56 | ppl 5288.63\n",
            "| epoch 8 | 1600/ 2149 batches | lr 0.10 | ms/batch 6.40 | loss 8.55 | ppl 5236.31\n",
            "| epoch 8 | 1800/ 2149 batches | lr 0.10 | ms/batch 6.32 | loss 8.55 | ppl 5248.91\n",
            "| epoch 8 | 2000/ 2149 batches | lr 0.10 | ms/batch 6.37 | loss 8.56 | ppl 5275.38\n",
            "-------------------------------------------------------------------\n",
            "| end of epoch 8 | time: 65.17s | valid loss 9.85 | valid ppl 19004.25\n",
            "-------------------------------------------------------------------\n",
            "Start Training\n",
            "| epoch 9 | 200/ 2149 batches | lr 0.10 | ms/batch 6.34 | loss 8.52 | ppl 5059.53\n",
            "| epoch 9 | 400/ 2149 batches | lr 0.10 | ms/batch 6.32 | loss 8.51 | ppl 5014.74\n",
            "| epoch 9 | 600/ 2149 batches | lr 0.10 | ms/batch 6.32 | loss 8.52 | ppl 5088.89\n",
            "| epoch 9 | 800/ 2149 batches | lr 0.10 | ms/batch 6.34 | loss 8.51 | ppl 5018.48\n",
            "| epoch 9 | 1000/ 2149 batches | lr 0.10 | ms/batch 6.32 | loss 8.50 | ppl 4934.01\n",
            "| epoch 9 | 1200/ 2149 batches | lr 0.10 | ms/batch 6.31 | loss 8.50 | ppl 4963.61\n",
            "| epoch 9 | 1400/ 2149 batches | lr 0.10 | ms/batch 6.32 | loss 8.53 | ppl 5132.85\n",
            "| epoch 9 | 1600/ 2149 batches | lr 0.10 | ms/batch 6.33 | loss 8.52 | ppl 5065.30\n",
            "| epoch 9 | 1800/ 2149 batches | lr 0.10 | ms/batch 6.32 | loss 8.51 | ppl 5014.60\n",
            "| epoch 9 | 2000/ 2149 batches | lr 0.10 | ms/batch 6.33 | loss 8.51 | ppl 5037.24\n",
            "-------------------------------------------------------------------\n",
            "| end of epoch 9 | time: 64.97s | valid loss 9.84 | valid ppl 18899.87\n",
            "-------------------------------------------------------------------\n",
            "Start Training\n",
            "| epoch 10 | 200/ 2149 batches | lr 0.10 | ms/batch 6.33 | loss 8.47 | ppl 4816.24\n",
            "| epoch 10 | 400/ 2149 batches | lr 0.10 | ms/batch 6.36 | loss 8.47 | ppl 4840.78\n",
            "| epoch 10 | 600/ 2149 batches | lr 0.10 | ms/batch 6.31 | loss 8.45 | ppl 4740.84\n",
            "| epoch 10 | 800/ 2149 batches | lr 0.10 | ms/batch 6.30 | loss 8.45 | ppl 4730.95\n",
            "| epoch 10 | 1000/ 2149 batches | lr 0.10 | ms/batch 6.35 | loss 8.46 | ppl 4763.14\n",
            "| epoch 10 | 1200/ 2149 batches | lr 0.10 | ms/batch 6.36 | loss 8.45 | ppl 4735.26\n",
            "| epoch 10 | 1400/ 2149 batches | lr 0.10 | ms/batch 6.32 | loss 8.44 | ppl 4677.00\n",
            "| epoch 10 | 1600/ 2149 batches | lr 0.10 | ms/batch 6.31 | loss 8.44 | ppl 4696.56\n",
            "| epoch 10 | 1800/ 2149 batches | lr 0.10 | ms/batch 6.31 | loss 8.46 | ppl 4749.05\n",
            "| epoch 10 | 2000/ 2149 batches | lr 0.10 | ms/batch 6.35 | loss 8.46 | ppl 4755.31\n",
            "-------------------------------------------------------------------\n",
            "| end of epoch 10 | time: 65.17s | valid loss 9.96 | valid ppl 21212.14\n",
            "-------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmOT3I-9xTtM"
      },
      "source": [
        "## Create MLP Model\n",
        "Multilayer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYbZUQuXxTJW"
      },
      "source": [
        "# source: https://www.oreilly.com/library/view/natural-language-processing/9781491978221/ch04.html\n",
        "# source: https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html\n",
        "# source: https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/01-basics/feedforward_neural_network/main.py#L37-L49\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MultilayerPerceptron(nn.Module):\n",
        "    def __init__(self, context_size, embedding_size, vocabulary_size):\n",
        "        super(MultilayerPerceptron, self).__init__()\n",
        "        self.fc1 = nn.Linear(context_size, embedding_size)\n",
        "        self.fc2 = nn.Linear(embedding_size, vocabulary_size)\n",
        "\n",
        "    def forward(self, x_in):\n",
        "        intermediate = F.relu(self.fc1(x_in))\n",
        "        output = self.fc2(intermediate)\n",
        "        return  F.softmax(output, dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lP76tQ69yDqi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75e08c5c-d438-474a-c86f-f42c0b31920a"
      },
      "source": [
        "batch_size = 3 # number of samples input at once\n",
        "input_dim = 6\n",
        "hidden_dim = 3\n",
        "output_dim = 50\n",
        "\n",
        "# Initialize model\n",
        "mlp = MultilayerPerceptron(input_dim, hidden_dim, output_dim)\n",
        "print(mlp)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MultilayerPerceptron(\n",
            "  (fc1): Linear(in_features=6, out_features=3, bias=True)\n",
            "  (fc2): Linear(in_features=3, out_features=50, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XJxiCnDydA5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b23535ee-5103-4f0e-de3e-9afae39dca81"
      },
      "source": [
        "def describe(x):\n",
        "    print(\"Type: {}\".format(x.type()))\n",
        "    print(\"Shape/size: {}\".format(x.shape))\n",
        "    print(\"Values: \\n{}\".format(x))\n",
        "\n",
        "###########################\n",
        "#Format                   #\n",
        "#Batch1: \n",
        "#[ [word1, word 2, tagert],\n",
        "#  [word1, word 2, tagert],\n",
        "#  [word1, word 2, tagert],\n",
        "#]\n",
        "\n",
        "x_input = torch.rand(batch_size, input_dim)\n",
        "describe(x_input)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Type: torch.FloatTensor\n",
            "Shape/size: torch.Size([3, 6])\n",
            "Values: \n",
            "tensor([[0.8654, 0.9656, 0.4177, 0.3434, 0.9115, 0.7048],\n",
            "        [0.2483, 0.4000, 0.4714, 0.8006, 0.0089, 0.3843],\n",
            "        [0.2623, 0.7188, 0.9339, 0.5699, 0.2633, 0.6035]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCzqQ1mXyfDV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96709b1e-be09-4a68-8ce0-83566ea87f9d"
      },
      "source": [
        "y_output = mlp(x_input)\n",
        "describe(y_output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Type: torch.FloatTensor\n",
            "Shape/size: torch.Size([3, 50])\n",
            "Values: \n",
            "tensor([[0.0239, 0.0413, 0.0146, 0.0167, 0.0166, 0.0132, 0.0379, 0.0147, 0.0109,\n",
            "         0.0185, 0.0095, 0.0062, 0.0070, 0.0320, 0.0161, 0.0112, 0.0138, 0.0076,\n",
            "         0.0473, 0.0206, 0.0067, 0.0398, 0.0180, 0.0255, 0.0126, 0.0543, 0.0143,\n",
            "         0.0131, 0.0216, 0.0137, 0.0202, 0.0120, 0.0078, 0.0069, 0.0112, 0.0141,\n",
            "         0.0108, 0.0115, 0.0291, 0.0367, 0.0293, 0.0426, 0.0154, 0.0271, 0.0158,\n",
            "         0.0189, 0.0142, 0.0425, 0.0091, 0.0256],\n",
            "        [0.0226, 0.0428, 0.0120, 0.0145, 0.0161, 0.0119, 0.0340, 0.0142, 0.0101,\n",
            "         0.0168, 0.0089, 0.0062, 0.0063, 0.0338, 0.0169, 0.0110, 0.0125, 0.0084,\n",
            "         0.0496, 0.0237, 0.0066, 0.0409, 0.0202, 0.0206, 0.0135, 0.0618, 0.0149,\n",
            "         0.0141, 0.0217, 0.0140, 0.0169, 0.0115, 0.0089, 0.0073, 0.0098, 0.0144,\n",
            "         0.0095, 0.0096, 0.0327, 0.0361, 0.0321, 0.0468, 0.0134, 0.0251, 0.0162,\n",
            "         0.0191, 0.0167, 0.0414, 0.0084, 0.0229],\n",
            "        [0.0221, 0.0435, 0.0111, 0.0137, 0.0157, 0.0113, 0.0329, 0.0140, 0.0097,\n",
            "         0.0163, 0.0087, 0.0061, 0.0060, 0.0347, 0.0173, 0.0109, 0.0121, 0.0086,\n",
            "         0.0506, 0.0247, 0.0065, 0.0409, 0.0209, 0.0191, 0.0138, 0.0642, 0.0151,\n",
            "         0.0145, 0.0218, 0.0139, 0.0158, 0.0113, 0.0093, 0.0074, 0.0093, 0.0143,\n",
            "         0.0090, 0.0090, 0.0343, 0.0362, 0.0334, 0.0487, 0.0129, 0.0245, 0.0164,\n",
            "         0.0191, 0.0177, 0.0408, 0.0081, 0.0221]], grad_fn=<SoftmaxBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "It4cDh0X8pFi"
      },
      "source": [
        "Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKBrTI8S8wBI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "outputId": "95494046-7852-4c3d-a9b2-a9702752ccd4"
      },
      "source": [
        "import datetime\n",
        "\n",
        "learning_rate = 20\n",
        "\n",
        "# Use cross entroy loss as loss function\n",
        "cross_entropy_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "# Use stochastic gradient descent as optimization function\n",
        "gradient_decent_optimiser = torch.optim.SGD(mlp.parameters(), lr=learning_rate)\n",
        "\n",
        "print('Training log:')\n",
        "num_batches = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "  print('Start Training')\n",
        "  epoch_start_time = datetime.datetime.now()\n",
        "\n",
        "  for i, (example_input, example_output) in enumerate(train_loader):\n",
        "    start_time = datetime.datetime.now()\n",
        "\n",
        "    # Forward pass\n",
        "    output = mlp(example_input)\n",
        "    loss = cross_entropy_loss(output, example_output)\n",
        "\n",
        "    # Backward Propagation and optimisation\n",
        "    gradient_decent_optimiser.zero_grad\n",
        "    loss.backward()\n",
        "    gradient_decent_optimiser.step()\n",
        "    \n",
        "    # Calculate time\n",
        "    end_time = datetime.datetim.now()\n",
        "    time_diff = (end_time - start_time)\n",
        "    time_diff_ms = time_diff * 1000\n",
        "\n",
        "    perplexity = torch.exp(loss)\n",
        "    if i == (num_batches - 1):\n",
        "      print('|epoch {}| {}/{} batches | lr {} | ms/batch {}| loss {:.2f}| ppl {:.2f}'.format(epoch+1, (i+1)*batch_size, num_batch, learning_rate, time_diff_ms, loss.item(), perplexity))\n",
        "  \n",
        "  for i, (valid_input, valid_output) in enumerate(valid_loader):\n",
        "    output = mlp(valid_input)\n",
        "    valid_loss = cross_entropy_loss(output, valid_output)\n",
        "    \n",
        "\n",
        "  epoch_end_time = datatime.datetime.now()\n",
        "  epoch_time = (epoch_end_time - epoch_start_time)\n",
        "  print('-------------------------------------------------------------------')\n",
        "  print('|end of epoch {}| time: {}s| valid loss {:.2f} | valid ppl {:.2f}'.format(epoch+1, epoch_time, valid_loss, valid_ppl))\n",
        "  print('-------------------------------------------------------------------')\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training log:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-612d8f829b39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training log:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mnum_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Start Training'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
          ]
        }
      ]
    }
  ]
}